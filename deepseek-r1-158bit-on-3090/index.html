<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=5" name="viewport"><meta content="yes" name="mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><title>使用 3090 部署 1.58bit 动态量化版 DeepSeek R1 671b</title><meta itemprop="title" content="使用 3090 部署 1.58bit 动态量化版 DeepSeek R1 671b - hsfzxjy 的博客"><meta itemprop="og:title" content="使用 3090 部署 1.58bit 动态量化版 DeepSeek R1 671b - hsfzxjy 的博客"><meta itemprop="image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="og:image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="description" content="1.58 bit 量化技术最早由论文 BitNet: Scaling 1-bit Transformers for..."><meta itemprop="og:type" content="website"><link rel="stylesheet" href="/dist/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head><body><nav class="nav"><span class="nav__toggler"><span class="nav__toggler-bar bar1"></span><span class="nav__toggler-bar bar2"></span><span class="nav__toggler-bar bar3"></span></span><span class="nav__core"><div class="nav__logo"><img src="/avatar.webp" loading="lazy"></div><a href="/" lang="zh" class="nav__brand">hsfzxjy</a></span><span class="nav__togglerClose"><span class="nav__togglerClose-circle"></span><span class="nav__togglerClose-bar bar1"></span><span class="nav__togglerClose-bar bar2"></span></span><div class="nav__pathIndicator"><a href="/categories/Tech/">Tech</a></div></nav><main class="mainContainer"><div lang="zh" class="post post-page"><h1 lang="zh" class="post__title">使用 3090 部署 1.58bit 动态量化版 DeepSeek R1 671b</h1><div class="post__meta font__ui">2025-02-22 | <span class="post__meta-categories"><a href="/categories/Tech/">Tech</a></span></div><div class="post__content font__body"><p>1.58 bit 量化技术最早由论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.11453">BitNet: Scaling 1-bit Transformers for Large Language Models</a> 提出，简单说即是将权值量化为仅有三个状态 -1、0 和 1，可最大程度地节约显存并加速推理。</p><p><a target="_blank" rel="noopener" href="https://unsloth.ai/blog/deepseekr1-dynamic">Run DeepSeek R1 Dynamic 1.58-bit</a> 一文介绍了 1.58 bit 动态量化版的 DeepSeek R1 671b（以下简称 unsloth 版）。我们知道 DeepSeek R1 70b 及以下的版本都是通过知识蒸馏得到的，唯有 671b 是满血的版本。然而原版 671b 的推理需要大量显存及算力，仅模型文件便有 404GB。</p><p>unsloth 版有选择地对部分权值作 1.58 bit 量化，将模型文件压缩至 131GB，同时也保持了不错的生成质量。结合 <a target="_blank" rel="noopener" href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> 实现的 CPU+GPU 混合推理，我们可在低成本的硬件上部署 671b 模型。</p><h2 id="使用-llama-bench-测试生成速度">使用 llama-bench 测试生成速度</h2><p>先说结论。笔者在实验室集群单个 3090 节点上尝试部署 unsloth 版 671b 模型，最大 token 生成速度可达 10 tokens/s，足够应付单人日常使用。节点的硬件配置如下：</p><ul><li><strong>CPU –</strong> AMD EPYC 7402 24-Core Processor</li><li><strong>RAM –</strong> 32GB x 16, DIMM DDR4 Synchronous Registered (Buffered) 3200 MHz</li><li><strong>GPU –</strong> 8x NVIDIA GeForce RTX 3090 (24GB VRAM)</li></ul><p>以下是 token 生成速度与 GPU Layers 数量的关系曲线：</p><p><object type="image/svg+xml" data="/assets/2025-02-07-deepseek-r1-158bit-on-3090/speed.svg"></object></p><p>GPU Layers 指置于 GPU 上的网络层数，取值范围为 0~62，可由 CLI 参数 <code>--n-gpu-layers</code> 配置。其中取 0 时代表完全使用 CPU 运行，取 62 时代表完全使用 GPU 运行。图中还标出了不同 GPU Layers 数量所需的 GPU 卡数（顶部横轴），以方便读者根据自己的显存大小调整 GPU Layers。</p><p>值得注意的是，虽然 7 张 3090 可以放下整个模型，在实际推理时考虑到 Context 的额外开销，所需显存可能不止于此，比如我后文部署 <code>--ctx-size=8192</code> 的 llama-server 时就需要占满 8 张 3090。</p><p>以上测试数据由如下脚本得出：</p><div class="gk-code hljs" data-gk-id="BLOCK1" data-gk-title="bench.sh"><div class="gk-code-display"><pre><span class="gk-section gk-zip zipped" data-gk-sid="BLOCK1.SEC1" data-gk-type="zip"><span class="line"><span class="hljs-built_in">export</span> CUDA_VISIBLE_DEVICES=0</span><br><span class="line">next_gpu=1</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> {0..62}; <span class="hljs-keyword">do</span></span><br><span class="line">    <span class="hljs-built_in">mkdir</span> -p benchout</span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>; <span class="hljs-keyword">do</span></span><br><span class="line">        ./llama.cpp/build/bin/llama-bench \</span><br><span class="line">            --model /models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \</span><br><span class="line">            --cache-type-k q4_0 \</span><br><span class="line">            --threads 64 --prio 2 \</span><br><span class="line">            --n-gpu-layers <span class="hljs-variable">$i</span> -n 128 2&gt;&amp;1 | <span class="hljs-built_in">tee</span> benchout/bench-<span class="hljs-variable">$i</span>.<span class="hljs-built_in">log</span></span><br><span class="line">        <span class="hljs-keyword">if</span> [ <span class="hljs-variable">${PIPESTATUS[0]}</span> -eq 0 ]; <span class="hljs-keyword">then</span></span><br><span class="line">            <span class="hljs-built_in">break</span></span><br><span class="line">        <span class="hljs-keyword">else</span></span><br><span class="line">            <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Retrying...&quot;</span></span><br><span class="line">            <span class="hljs-built_in">export</span> CUDA_VISIBLE_DEVICES=<span class="hljs-variable">${CUDA_VISIBLE_DEVICES}</span>,<span class="hljs-variable">${next_gpu}</span></span><br><span class="line">            next_gpu=$((next_gpu + <span class="hljs-number">1</span>))</span><br><span class="line">        <span class="hljs-keyword">fi</span></span><br><span class="line">    <span class="hljs-keyword">done</span></span><br><span class="line"><span class="hljs-keyword">done</span></span><br></span></pre></div></div><p>下面介绍如何借助 llama.cpp 运行一个基础可用的 Web 界面，以使用 unsloth 版 671b 模型。</p><h2 id="llama-server-服务部署">llama-server 服务部署</h2><h3 id="模型下载">模型下载</h3><p>本次使用的模型位于 Huggingface 的 <a target="_blank" rel="noopener" href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S">unsloth/DeepSeek-R1-GGUF 仓库</a>。仓库提供了多种量化的版本，我们只需下载其中的 <code>DeepSeek-R1-UD-IQ1_S</code> 目录，置于 <code>/mnt/models/</code> 路径，形成以下结构：</p><div class="gk-code hljs" data-gk-id="BLOCK2"><div class="gk-code-display"><pre><span class="line">/mnt/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S</span><br><span class="line">├── DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf</span><br><span class="line">├── DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf</span><br><span class="line">└── DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf</span><br></pre></div></div><p>下载模型可使用 Python 的 <code>huggingface_hub</code> 库：</p><div class="gk-code hljs" data-gk-id="BLOCK3" data-gk-title="Download with Python"><div class="gk-code-display"><pre><span class="gk-section gk-zip zipped" data-gk-sid="BLOCK3.SEC1" data-gk-type="zip"><span class="line"><span class="hljs-keyword">import</span> os</span><br><span class="line">os.environ[<span class="hljs-string">&quot;HF_HUB_ENABLE_HF_TRANSFER&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span></span><br><span class="line"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download</span><br><span class="line">snapshot_download(</span><br><span class="line">  repo_id = <span class="hljs-string">&quot;unsloth/DeepSeek-R1-GGUF&quot;</span>,</span><br><span class="line">  local_dir = <span class="hljs-string">&quot;/mnt/models&quot;</span>,</span><br><span class="line">  allow_patterns = [<span class="hljs-string">&quot;*UD-IQ1_S*&quot;</span>],</span><br><span class="line">)</span><br></span></pre></div></div><p>当然，如果环境中不方便使用 Python，笔者推荐使用 <a target="_blank" rel="noopener" href="https://github.com/bodaay/HuggingFaceModelDownloader">HuggingFaceModelDownloader</a> 下载。这是一个用 Go 编写的下载器，无需像 Python 那样配置环境即可使用：</p><div class="gk-code hljs" data-gk-id="BLOCK4" data-gk-title="Download with hfd"><div class="gk-code-display"><pre><span class="line">curl -sSL https://g.bodaay.io/hfd -o hfd &amp;&amp; <span class="hljs-built_in">chmod</span> +x hfd</span><br><span class="line">./hfd -m unsloth/DeepSeek-R1-GGUF:UD-IQ1_S -c 8 -s /mnt/models</span><br></pre></div></div><h3 id="编译-llamacpp-及启动-llama-server-服务">编译 llama.cpp 及启动 llama-server 服务</h3><p>运行 1.58 bit 的 DeepSeek R1 需要使用 <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>。笔者选择使用 Docker 部署，以避免繁琐的环境配置。</p><p>为了方便读者使用，笔者将镜像的准备和启动过程整合到了 Docker Compose 文件中。读者只需将以下两个文件 <code>compose.yaml</code> 和 <code>llama_Dockerfile</code> 放在同一个目录下，执行 <code>docker compose up</code> 即可启动 llama-server。llama-server 内置了一个简单的 Web 界面，可以通过 <code>http://localhost:10000</code> 访问。</p><div class="gk-unified-code tab" data-gk-style="tab"><div class="gk-code hljs" data-gk-id="docker-compose" data-gk-title="compose.yaml"><div class="gk-code-display"><pre><span class="line"><span class="hljs-attr">services:</span></span><br><span class="line">  <span class="hljs-attr">deepseek:</span></span><br><span class="line">    <span class="hljs-attr">build:</span></span><br><span class="line">      <span class="hljs-attr">args:</span></span><br><span class="gk-section gk-section" data-gk-sid="docker-compose.SEC1" data-gk-type="section" data-gk-desc="如果你访问 GitHub 需要代理，在这里设置，如 PROXY=http://127.0.0.1:1081" data-gk-desc-show="true"><span class="line">        <span class="hljs-bullet">-</span> <span class="hljs-string">PROXY=</span></span><br></span><span class="gk-section gk-section" data-gk-sid="docker-compose.SEC2" data-gk-type="section" data-gk-desc="html:根据自己的 GPU 型号设置 arch 值，如 RTX 3090 对应 86。&lt;br&gt;详细列表可在这里查询 &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus&quot; target=_blank&gt;CUDA GPUs&lt;/a&gt;" data-gk-desc-show="true"><span class="line">        <span class="hljs-bullet">-</span> <span class="hljs-string">CUDA_ARCH=86</span></span><br></span><span class="line">      <span class="hljs-attr">dockerfile:</span> <span class="hljs-string">llama_Dockerfile</span></span><br><span class="line">    <span class="hljs-attr">command:</span> <span class="hljs-string">|</span></span><br><span class="line"><span class="hljs-string">      llama-server</span></span><br><span class="line"><span class="hljs-string">      --model /models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf</span></span><br><span class="line"><span class="hljs-string">      --cache-type-k q4_0</span></span><br><span class="line"><span class="hljs-string">      --threads 12 --prio 2</span></span><br><span class="line"><span class="hljs-string">      --temp 0.6</span></span><br><span class="line"><span class="hljs-string">      --ctx-size 8192</span></span><br><span class="line"><span class="hljs-string">      --port 10000</span></span><br><span class="line"><span class="hljs-string">      --host 0.0.0.0</span></span><br><span class="gk-section gk-section" data-gk-sid="docker-compose.SEC3" data-gk-type="section" data-gk-desc="offload 到 GPU 上的网络层数，取值 0~62，根据自己 VRAM 大小调整" data-gk-desc-show="true"><span class="line"><span class="hljs-string">      --n-gpu-layers 62</span></span><br></span><span class="line"><span class="hljs-string"></span>    <span class="hljs-attr">volumes:</span></span><br><span class="line">      <span class="hljs-bullet">-</span> <span class="hljs-string">/mnt/models:/models</span></span><br><span class="line">    <span class="hljs-attr">ports:</span></span><br><span class="line">      <span class="hljs-bullet">-</span> <span class="hljs-number">10000</span><span class="hljs-string">:10000</span></span><br><span class="line">    <span class="hljs-attr">deploy:</span> { <span class="hljs-attr">resources:</span> { <span class="hljs-attr">reservations:</span> { <span class="hljs-attr">devices:</span> { <span class="hljs-attr">driver:</span> <span class="hljs-string">nvidia</span>, <span class="hljs-attr">capabilities:</span> [ <span class="hljs-string">gpu</span> ],</span><br><span class="gk-section gk-section" data-gk-sid="docker-compose.SEC4" data-gk-type="section" data-gk-desc="根据实际情况调整 GPU 数量" data-gk-desc-show="true"><span class="line">      <span class="hljs-attr">count:</span> <span class="hljs-number">8</span></span><br></span><span class="line">    } } } }</span><br></pre></div></div><div class="gk-code hljs" data-gk-id="dockerfile" data-gk-title="llama_Dockerfile"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> hsfzxjy/devkit:cuda11.<span class="hljs-number">7.1</span></span><br><span class="line"><span class="hljs-keyword">ARG</span> PROXY</span><br><span class="line"><span class="hljs-keyword">ENV</span> http_proxy=${PROXY} https_proxy=${PROXY}</span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">mkdir</span> /app/ &amp;&amp; <span class="hljs-built_in">cd</span> /app &amp;&amp; git <span class="hljs-built_in">clone</span> https://github.com/ggerganov/llama.cpp</span></span><br><span class="line"><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app/llama.cpp</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get install -y libcurl4-openssl-dev</span></span><br><span class="line"><span class="hljs-keyword">ARG</span> CUDA_ARCH</span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> cmake . -B build \</span></span><br><span class="line"><span class="language-bash">    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON -DCMAKE_CUDA_ARCHITECTURES=<span class="hljs-variable">${CUDA_ARCH}</span></span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> cmake --build build --config Release -j16 --clean-first --target llama-cli llama-server llama-bench</span></span><br><span class="line"><span class="hljs-keyword">ENV</span> PATH=/app/llama.cpp/build/bin:${PATH}</span><br><span class="line"><span class="hljs-keyword">ENV</span> http_proxy= https_proxy=</span><br></pre></div></div></div><br><blockquote><p class="cc">作者：hsfzxjy<br>链接：<span class="cc-link"></span><br>许可：<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a>.<br>著作权归作者所有。本文<b>不允许</b>被用作商业用途，非商业转载请注明出处。</p></blockquote><script>!function(){var n=document.querySelector("span.cc-link");n&&(n.innerHTML='<a href="'+location.href+'">'+location.href+"</a>")}()</script></div><div class="post__tags"><a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/DeepSeek/">DeepSeek</a><a href="/tags/Quantization/">Quantization</a><a href="/tags/RTX-3090/">RTX 3090</a><a href="/tags/llama-cpp/">llama.cpp</a></div><div class="post-nav"><a href="/configure-http-proxy-in-devcontainer/" class="next">如何在 VS Code DevContainer 中配置 HTTP 代理»</a></div><div id="disqus_thread"><div id="no-comment"><h2>OOPS!</h2><span>A comment box should be right here...</span><span>But it was gone due to network issues :-(</span><span>If you want to leave comments, make sure you have access to&nbsp;<a target="_blank" rel="noopener" href="https://disqus.com">disqus.com</a>.</span></div><script>var disqus_shortname="hsfzxjy",disqus_identifier="deepseek-r1-158bit-on-3090/",disqus_title="使用 3090 部署 1.58bit 动态量化版 DeepSeek R1 671b",disqus_url="https://i.hsfzxjy.site/deepseek-r1-158bit-on-3090/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.defer=!0,e.src="https://"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("body")[0]||document.getElementsByTagName("head")[0]).appendChild(e),e.onerror=function(){document.getElementById("no-comment").classList.add("show")}}()</script><script id="dsq-count-scr" src="//hsfzxjy.disqus.com/count.js" async defer></script></div></div></main><section class="aside__group"><span class="aside__btn"><div lang="zh" class="github-btn"><a href="#" rel="noopener noreferrer" target="_blank" class="gh-btn"><span class="gh-ico"></span><span class="gh-text">FOLLOW ME</span></a><a href="#" rel="noopener noreferrer" target="_blank" class="gh-count"></a></div><script>window.GH_BUTTON={username:"hsfzxjy"}</script></span><aside class="aside__left"><div class="aside__menuList"><span class="aside__menuList-item__icon"><i class="icon-home"></i></span><a href="/" class="aside__menuList-item"><span lang="zh" class="font__ui">首页 / Home</span></a><span class="aside__menuList-item__icon current"><i class="icon-embed2"></i></span><a href="/categories/Tech/" class="aside__menuList-item current"><span lang="zh" class="font__ui">科技 / Tech</span></a><span class="aside__menuList-item__icon"><i class="icon-android"></i></span><a href="/categories/Soliloquy/" class="aside__menuList-item"><span lang="zh" class="font__ui">呓语 / Soliloquy</span></a><span class="aside__menuList-item__icon"><i class="icon-leaf"></i></span><a href="/categories/Life/" class="aside__menuList-item"><span lang="zh" class="font__ui">生活 / Life</span></a><span class="aside__menuList-item__icon"><i class="icon-bookmarks"></i></span><a href="/categories/Memo/" class="aside__menuList-item"><span lang="zh" class="font__ui">速记 / Memo</span></a><span class="aside__menuList-item__icon"><i class="icon-books"></i></span><a href="/categories/Series/" class="aside__menuList-item"><span lang="zh" class="font__ui">连载 / Series</span></a><span class="aside__menuList-item__icon"><i class="icon-sigma"></i></span><a href="/works/" class="aside__menuList-item"><span lang="zh" class="font__ui">项目 / Projects</span></a><span class="aside__menuList-item__icon"><i class="icon-earth"></i></span><a href="/links/" class="aside__menuList-item"><span lang="zh" class="font__ui">友链 / Links</span></a><span class="aside__menuList-item__icon"><i class="icon-wink"></i></span><a href="/about/" class="aside__menuList-item"><span lang="zh" class="font__ui">关于 / About</span></a><span class="aside__menuList-item__icon"><i class="icon-history"></i></span><a href="/aggr/" class="aside__menuList-item"><span lang="zh" class="font__ui">索引 / Index</span></a><span class="aside__menuList-item__icon"><i class="icon-rss2"></i></span><a href="/rss.xml" class="aside__menuList-item"><span lang="zh" class="font__ui">订阅 / RSS</span></a></div></aside><aside class="aside__right"><div id="toc" lang="zh" class="font__body"><div class="toc-toggler"></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-llama-bench-%E6%B5%8B%E8%AF%95%E7%94%9F%E6%88%90%E9%80%9F%E5%BA%A6"><span class="toc-number">1.</span> <span class="toc-text">使用 llama-bench 测试生成速度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#llama-server-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2"><span class="toc-number">2.</span> <span class="toc-text">llama-server 服务部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.1.</span> <span class="toc-text">模型下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91-llamacpp-%E5%8F%8A%E5%90%AF%E5%8A%A8-llama-server-%E6%9C%8D%E5%8A%A1"><span class="toc-number">2.2.</span> <span class="toc-text">编译 llama.cpp 及启动 llama-server 服务</span></a></li></ol></li></ol></div></div></aside></section><div id="footer" style="text-align:center" lang="zh" class="font__ui">© <a href="/" rel="nofollow">hsfzxjy 的博客.</a>&nbsp;Powered by&nbsp;<a rel="nofollow" target="_blank" href="https://hexo.io">Hexo.</a>&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy/byak-hexo">Theme</a>&nbsp;by&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy">hsfzxjy</a>.<div style="margin-top:10px"><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011202001249" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/img/beian.png" style="float:left" loading="lazy"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤公网安备44011202001249号</span></a><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px" href="http://beian.miit.gov.cn/"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备2020075702号-1</span></a></div></div></body><script src="/dist/js/main.js" async defer></script></html>