<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=5" name="viewport"><meta content="yes" name="mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><script>(()=>{let e="/cnfonts.js?rDuukLHQ",t=document,n=navigator.serviceWorker,r=!1;var a;(r=n&&(n.register(e,{scope:"/"}).then(e=>e.update()),a=n.controller)&&a.scriptURL.endsWith(e)&&"activated"===a.state?!0:r)?((a=t.createElement("link")).rel="stylesheet",a.href="/fontchan/rDuukLHQ.css",a.blocking="render",t.head.appendChild(a)):((a=t.createElement("script")).src=e,a.onload=()=>$fontchan.injectCss(),t.head.appendChild(a))})()</script><title>Diving from the CUDA Error 804 into a bug of libnvidia-container</title><meta itemprop="title" content="Diving from the CUDA Error 804 into a bug of libnvidia-container - hsfzxjy 的博客"><meta itemprop="og:title" content="Diving from the CUDA Error 804 into a bug of libnvidia-container - hsfzxjy 的博客"><meta itemprop="image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="og:image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="description" content="Several users reported to encounter &amp;quot;Error 804: forw..."><meta itemprop="og:type" content="website"><link rel="stylesheet" href="/dist/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head><body><nav class="nav"><span class="nav__toggler"><span class="nav__toggler-bar bar1"></span><span class="nav__toggler-bar bar2"></span><span class="nav__toggler-bar bar3"></span></span><span class="nav__core"><div class="nav__logo"><img src="/avatar.webp" loading="lazy"></div><a href="/" lang="zh" class="nav__brand">hsfzxjy</a></span><span class="nav__togglerClose"><span class="nav__togglerClose-circle"></span><span class="nav__togglerClose-bar bar1"></span><span class="nav__togglerClose-bar bar2"></span></span><div class="nav__pathIndicator"><a href="/categories/Tech/">Tech</a></div></nav><main class="mainContainer"><div lang="en" class="post post-page"><h1 lang="en" class="post__title">Diving from the CUDA Error 804 into a bug of libnvidia-container</h1><div class="post__meta font__ui">2023-03-19 | <span class="post__meta-categories"><a href="/categories/Tech/">Tech</a></span></div><div class="post__content font__body"><p>Several users reported to encounter <code>&quot;Error 804: forward compatibility was attempted on non supported HW&quot;</code> during the usage of some customized PyTorch docker images on our GPU cluster.</p><p>At first glance I recognized the culprit to be a version mismatch between installed driver on the host and required driver in the image. The corrupted images as they described were built targeting <code>CUDA == 11.3</code> with a corresponding driver version <code>== 465</code> <sup id="fnref:1"><a href="#fn:1">1</a></sup>, while some of our hosts are shipped with driver version <code>460</code>. As a solution I told them to downgrade the targeting CUDA version by choosing a base image such as <code>nvidia/cuda:11.2.0-devel-ubuntu18.04</code>, which indeed well solved the problem.</p><p>But later on I suspected the above hypothesis being the real cause. An observed counterexample was that another line of docker images targeting even higher CUDA version would run normally on those hosts, for example, the latest <code>ghcr.io/pytorch/pytorch:2.0.0-devel</code> built for <code>CUDA == 11.7</code>. This won’t be the case if CUDA version mismatch truly matters.</p><p>Afterwards I did a bit of research concerning the problem and learnt some interesting stuff which this post is going to share. In short, <strong>the recently released minor version compatibility allows applications built for newer CUDA to run on machines with some older drivers, but libnvidia-container doesn’t correcly handle it due to <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/libnvidia-container/issues/138">a bug</a> and eventually leads to such an error</strong>.</p><p>Towards thorough comprehension, this post will first introduce the constitution of CUDA components, following with the compatibility policy of different components, and finally unravel the bug and devise a workaround for it. But before diving deep, I’ll give two Dockerfile samples to illustrate the problem.</p><h1 id="reproduction-samples">Reproduction Samples</h1><p>The host reported as problematic has 8x GeForce RTX 3090 with driver version <code>460.67</code> and CUDA <code>11.2</code>. Here is an image with <code>torch == 1.12.1</code> built for CUDA <code>11.3</code> and fails on the host:</p><div class="gk-code hljs" data-gk-id="dockerfile_bad_old" data-gk-title="Dockerfile_bad"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.3</span>.<span class="hljs-number">0</span>-cudnn8-devel-ubuntu20.<span class="hljs-number">04</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt update -y &amp;&amp; apt install -y python3 python3-pip</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</span></span><br><span class="line"><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;import torch; print(torch.rand(2, 3).cuda())&quot;</span>]</span></span><br></pre></div></div><p class="par">By contrast below is an image with <code>torch == 2.0.0</code> built for CUDA <code>11.7</code> and runs normally:</p><div class="gk-code hljs" data-gk-id="BLOCK1" data-gk-title="Dockerfile_good"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> ghcr.io/pytorch/pytorch:<span class="hljs-number">2.0</span>.<span class="hljs-number">0</span>-devel</span><br><span class="line"><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;import torch; print(torch.rand(2, 3).cuda())&quot;</span>]</span></span><br></pre></div></div><p class="par">For convenience I also write a <code>Makefile</code> to combine the process of building and running either image:</p><div class="gk-code hljs" data-gk-id="BLOCK2" data-gk-title="Makefile"><div class="gk-code-display"><pre><span class="line"><span class="hljs-section">good:</span></span><br><span class="line">      docker build -t good -&lt; Dockerfile_good</span><br><span class="line">      docker run --gpus=&#x27;<span class="hljs-string">&quot;device=0&quot;</span>&#x27; --rm -it good</span><br><span class="line"><span class="hljs-section">bad:</span></span><br><span class="line">      docker build -t bad -&lt; Dockerfile_bad</span><br><span class="line">      docker run --gpus=&#x27;<span class="hljs-string">&quot;device=0&quot;</span>&#x27; --rm -it bad</span><br></pre></div></div><p class="par">With the <code>Makefile</code> you can run <code>make good</code> or <code>make bad</code> to see respective results:</p><div class="gk-code hljs" data-gk-id="BLOCK3"><div class="gk-code-display"><pre><span class="line"><span class="hljs-meta prompt_">$ </span><span class="language-bash">make good</span></span><br><span class="line">tensor([[0.1245, 0.2403, 0.9967],</span><br><span class="line">        [0.5950, 0.1597, 0.1985]], device=&#x27;cuda:0&#x27;)</span><br><span class="line"><span class="hljs-meta prompt_">$ </span><span class="language-bash">make bad</span></span><br><span class="line">&lt;string&gt;:1: UserWarning: Failed to initialize NumPy: numpy.core.multiarray failed to import (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">  File &quot;/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py&quot;, line 217, in _lazy_init</span><br><span class="line">    torch._C._cuda_init()</span><br><span class="line">RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW</span><br><span class="line">make: *** [bad] Error 1</span><br></pre></div></div><p>We start off touring from the constitution of CUDA.</p><h1 id="components-of-cuda">Components of CUDA</h1><p>When talked about the term “CUDA”, two concepts “CUDA Toolkit” and “NVIDIA Display Drivers” are usually mixed up. This figure <sup id="fnref:2"><a href="#fn:2">2</a></sup> illustrates their distinction as well as the cascading relationship:</p><p class="par"><img loading="lazy" src="https://docs.nvidia.com/deploy/cuda-compatibility/graphics/CUDA-components.png" alt="Components of CUDA"></p><p class="par">The driver at low level bridges the communication between softwares and underlying NVIDIA hardwares. The toolkit instead lies at a higher level to provide convenience for easy GPU programming.</p><p>If we take a closer look at the driver, we can see it decomposed into two secondary components “user-mode driver or UMD (<code>libcuda.so</code>)” and “kernel-mode driver or KMD (<code>nvidia.ko</code>)”. The KMD runs in OS kernel to do the most intimate contact with the hardware, while the UMD as an abstraction provides API to communicate with the kernel driver.</p><p>Generally, the applications compiled by CUDA toolkit will dynamically search and link <code>libcuda.so</code> during starting, which under the hood dispatches user requests to the kernel as illustrated below:</p><figure class="graphviz"><svg xmlns="http://www.w3.org/2000/svg" width="640" height="56pt" viewBox="0 0 480.44 55.76"><g class="graph"><path fill="none" d="M0 55.76V0h480.44v55.76z"/><g class="node" transform="translate(4 51.76)"><path fill="#fff5ee" stroke="#444" d="m50.1-8.18 2.81-.08 2.78-.13 2.74-.17 2.69-.22 2.63-.25 2.55-.3 2.47-.33 2.38-.38 2.28-.41 2.17-.45 2.05-.48 1.93-.52 1.8-.55 1.67-.59 1.52-.61 1.38-.64 1.24-.67 1.09-.69.94-.71.78-.74.64-.75.49-.77.34-.79.2-.79.05-.81-.09-.81-.22-.82-.35-.83-.48-.82-.6-.83-.71-.82-.83-.81-.92-.81-1.03-.79-1.11-.79-1.2-.77-1.28-.75-1.35-.74-1.42-.71-1.47-.69-1.54-.67-1.58-.64-1.62-.61-1.67-.59-1.7-.55-1.73-.52-1.75-.48-1.78-.45-1.8-.41-1.82-.38-1.83-.33-1.84-.3-1.86-.25-1.86-.22-1.86-.17-1.87-.13-1.88-.08-1.87-.05h-1.88l-1.87.05-1.88.08-1.87.13-1.86.17-1.86.22-1.86.25-1.84.3-1.83.33-1.82.38-1.8.41-1.78.45-1.75.48-1.73.52-1.7.55-1.67.59-1.63.61-1.58.64-1.53.67-1.48.69-1.41.71-1.35.74-1.28.75-1.2.77-1.11.79-1.03.79-.92.81-.83.81-.71.82-.6.83-.48.82-.35.83-.22.82-.09.81.05.81.2.79.34.79.49.77.63.75.79.74.94.71 1.09.69 1.23.67 1.39.64 1.52.61 1.67.59 1.8.55 1.93.52 2.05.48 2.17.45 2.28.41 2.38.38 2.47.33 2.55.3 2.63.25 2.69.22 2.74.17 2.78.13 2.81.08 2.82.05h2.84z"/><text x="22.61" y="-19.83" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">binaries</text></g><g class="node" transform="translate(4 51.76)"><path fill="#fff5ee" stroke="#444" d="m223.51-.07 3.75-.13 3.7-.19 3.65-.26 3.59-.33 3.5-.38 3.4-.45 3.29-.51 3.17-.57 3.04-.62 2.89-.68 2.73-.74 2.58-.79 2.39-.83 2.22-.89 2.03-.93 1.85-.97 1.64-1.01 1.45-1.05 1.25-1.08 1.05-1.12.85-1.14.65-1.17.46-1.19.26-1.2.07-1.23-.12-1.23-.29-1.25-.47-1.24-.64-1.26-.8-1.24-.95-1.25-1.1-1.23-1.23-1.23-1.37-1.2-1.48-1.19-1.6-1.17-1.7-1.14-1.8-1.12-1.89-1.08-1.97-1.05-2.04-1.01-2.11-.97-2.16-.93-2.22-.89-2.27-.83-2.3-.79-2.34-.74-2.37-.68-2.4-.62-2.42-.57-2.44-.51-2.46-.45-2.47-.38-2.48-.33-2.48-.26-2.49-.19-2.5-.13-2.5-.07h-2.5l-2.5.07-2.5.13-2.49.19-2.49.26-2.47.33-2.47.38-2.46.45-2.44.51-2.42.57-2.4.62-2.37.68-2.34.74-2.31.79-2.26.83-2.22.89-2.17.93-2.1.97-2.04 1.01-1.97 1.05-1.89 1.08-1.8 1.12-1.7 1.14-1.6 1.17-1.48 1.19-1.37 1.2-1.23 1.23-1.1 1.23-.95 1.25-.8 1.24-.64 1.26-.47 1.24-.29 1.25-.12 1.23.07 1.23.26 1.2.45 1.19.66 1.17.85 1.14 1.04 1.12 1.25 1.08 1.45 1.05 1.65 1.01 1.84.97 2.04.93 2.21.89 2.4.83 2.57.79 2.74.74 2.89.68 3.04.62 3.17.57 3.29.51 3.4.45 3.5.38 3.58.33 3.66.26 3.7.19 3.74.13 3.77.07h3.78z"/><text x="188.61" y="-25.58" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">libcuda.so</text><text x="184.48" y="-11.58" fill="#444" font-family="Times New Roman,serif" font-size="14">[user-mode]</text></g><g class="edge" transform="translate(4 51.76)"><path fill="none" stroke="#444" d="M91.05-23.88H146"/><path fill="#444" stroke="#444" d="m145.93-27.38 10 3.5-10 3.5z"/><text x="124.23" y="-28.63" fill="#444" font-family="Times New Roman,serif" font-size="10" text-anchor="middle">link against</text></g><g class="node" transform="translate(4 51.76)"><path fill="#fff5ee" stroke="#444" d="m409.3-.07 4.26-.13 4.22-.19 4.16-.26 4.08-.33 3.98-.38 3.88-.45 3.75-.51 3.6-.57 3.46-.62 3.29-.68 3.12-.74 2.92-.79 2.74-.83 2.52-.89 2.32-.93 2.09-.97 1.88-1.01 1.65-1.05 1.42-1.08 1.2-1.12.97-1.14.74-1.17.51-1.19.3-1.2.08-1.23-.13-1.23-.34-1.25-.53-1.24-.73-1.26-.91-1.24-1.08-1.25-1.25-1.23-1.41-1.23-1.55-1.2-1.69-1.19-1.82-1.17-1.94-1.14-2.05-1.12-2.15-1.08-2.24-1.05-2.32-1.01-2.4-.97-2.47-.93-2.52-.89-2.58-.83-2.63-.79-2.67-.74-2.7-.68-2.73-.62-2.75-.57-2.78-.51-2.8-.45-2.81-.38-2.82-.33-2.83-.26-2.84-.19-2.84-.13-2.85-.07h-2.84l-2.85.07-2.84.13-2.84.19-2.83.26-2.82.33-2.81.38-2.8.45-2.78.51-2.75.57-2.74.62-2.7.68-2.66.74-2.63.79-2.58.83-2.52.89-2.47.93-2.4.97-2.32 1.01-2.24 1.05-2.15 1.08-2.05 1.12-1.94 1.14-1.82 1.17-1.69 1.19-1.55 1.2-1.41 1.23L337-27l-1.08 1.25-.91 1.24-.73 1.26-.53 1.24-.34 1.25-.13 1.23.08 1.23.3 1.2.51 1.19.74 1.17.97 1.14 1.2 1.12 1.42 1.08 1.65 1.05 1.87 1.01 2.1.97 2.32.93 2.52.89 2.73.83 2.93.79 3.12.74 3.29.68 3.45.62 3.61.57 3.75.51 3.88.45 3.98.38 4.08.33 4.16.26 4.22.19 4.26.13 4.29.07h4.3z"/><text x="375.86" y="-25.58" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">nvidia.ko</text><text x="363.86" y="-11.58" fill="#444" font-family="Times New Roman,serif" font-size="14">[kernel-mode]</text></g><g class="edge" transform="translate(4 51.76)"><path fill="none" stroke="#444" d="M278.18-23.88h44.74"/><path fill="#444" stroke="#444" d="m322.77-27.38 10 3.5-10 3.5z"/><text x="306.12" y="-28.63" fill="#444" font-family="Times New Roman,serif" font-size="10" text-anchor="middle">talk with</text></g></g></svg></figure><p class="nomargin">So far so good, if only the compiler in toolkit agrees on APIs with the targeting driver.</p><p>Sadly, that is not the norm. In real world, developers compile the programs on one machine and dispatch them to run on others, expecting those programs compiled by a specific version of CUDA toolkit could run on a wide variety of hardwares, or otherwise users would complain about the corrupted binaries.</p><p>Towards this guarantee, several compatibility policies are induced.</p><h1 id="cuda-compatibility-policies">CUDA Compatibility Policies</h1><p>Before we introduce the policies, we should know about how the components are versioned. The CUDA toolkit and the drivers adopt different version schemes, with the toolkit versioned like <code>11.2</code> and drivers like <code>460.65</code>. Therefore, “driver 460.65” refers to the version of <code>libcuda.so</code> and <code>nvidia.ko</code>; similarly, when somebody says “CUDA 11.2”, it’s <em>the toolkit version</em> being mentioned.</p><p>NVIDIA devises multiple rules to ensure user binaries would work on a wide range of driver-hardware combinations, which can be grouped into two categories, i.e., <strong>toolkit-driver compatibility</strong> and <strong>UMD-KMD compatibility</strong>.</p><h2 id="toolkit-driver-compatibility">Toolkit-driver compatibility</h2><p>These policies constrain that <strong>binaries compiled by a specific CUDA toolkit can run on what version of driver</strong>.</p><p>Basically we have the <strong>“Backward Compatibility”</strong>. Each CUDA toolkit has a so-called toolkit driver version <sup id="fnref:3"><a href="#fn:3">3</a></sup>. Binaries compiled by that toolkit are guaranteed to run on drivers newer than the toolkit driver version. For example, the toolkit driver version of CUDA 11.2 is 460.27.03, which means binaries compiled by CUDA 11.2 should work on any driver &gt;= 460.27.03. This is the most fundamental and agelong policy.</p><p>From CUDA 11 onwards, another policy named <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#minor-version-compatibility"><strong>“Minor Version Compatibility”</strong></a> <sup id="fnref:4"><a href="#fn:4">4</a></sup> was proposed. This policy allows binaries compiled by toolkits with the same major version to a the same driver version requirement. For example, binaries compiled by CUDA 11.0 would work on driver &gt;= 450.36.06. Since CUDA 11.2 has the same major version with CUDA 11.0, binaries compiled by CUDA 11.2 could also work on driver &gt;= 450.36.06 <sup id="fnref:5"><a href="#fn:5">5</a></sup>.</p><p>The backward compatibility ensures compiled binaries would work on machines shipped with drivers of future version, while the minor version compatibility reduces the necessity of upgrading drivers to run some newly compiled binaries. Generally, a binary compiled by CUDA toolkit $X.Y$ should work with driver with version $M$, if either of the following satisfies:</p><ol><li>CUDA toolkit $X.Y$ has toolkit driver version $N$ and $M \geq N$;</li><li>$X \geq 11$ and a CUDA toolkit $X.Y_2$ has toolkit driver version $N_2$ and $M \geq N_2$.</li></ol><p>However, the above policies only consider the relationship between CUDA toolkit and drivers. What if the user-mode and kernel-mode drivers have diverged version? This is where <strong>UMD-KMD compatibility</strong> applies.</p><h2 id="umd-kmd-compatibility">UMD-KMD compatibility</h2><p>In ideal case, kernel-mode driver should always work with user-mode driver with the same version. But upgrading kernel-mode drivers is sometimes tricky and troublesome, of which some users such as data center admins could not take the risk. Towards this consideration, NVIDIA devised the <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#forward-compatibility-title"><strong>“Forward Compatibility”</strong></a> to allow old-versioned KMD to cooperate with new-versioned UMD under some circumstance.</p><p>Specifically, a kernel-mode driver would support all user-mode drivers releases during its lifetime. For instance, the driver 418.x has end of life (EOL) in March 2022, before which driver 460.x was released, then KMD 418.x would work with UMD 460.x. The compatibility does not involve anything at a higher level such as CUDA toolkit.</p><p>It’s worth noting that, this policy <strong>does not apply to all GPU hardwares</strong> but only a fraction of them. NVIDIA has limited forward compatibility to be applicable for systems with NVIDIA Data Center GPUs (the Tesla branch) or <a target="_blank" rel="noopener" href="https://docs.nvidia.com/ngc/ngc-deploy-on-premises/ngc-ready-systems/index.html">NGC Server Ready</a> SKUs of RTX cards <sup id="fnref:6"><a href="#fn:6">6</a></sup>. If you own a GeForce RTX 3090, like in my scenario, you won’t enjoy this stuff.</p><h2 id="summary-of-compatibility">Summary of Compatibility</h2><p>Let’s make a quick review for the various types of compatibility policies. If you have a binary compiled by CUDA $X.Y$, a host with UMD (<code>libcuda.so</code>) versioned $M$ and KMD (<code>nvidia.ko</code>) versioned $M&#39;$, then they would work fine if both of the two conditions hold:</p><ol><li>The UMD and KMD is compatible. Specifically, either<ol><li>the GPU supports forward compatibility (Tesla branch or NGC ready), and driver $M$ comes before the EOL of driver $M&#39;$ (<strong>the forward compatibility</strong>); or</li><li>$M = M&#39;$.</li></ol></li><li>The CUDA toolkit and UMD is compatible. Specifically, either<ol><li>CUDA toolkit $X.Y$ has toolkit driver version $N$ and $M \geq N$ (<strong>the backward compatibility</strong>); or</li><li>major version $X \geq 11$ and there exists another toolkit $X.Y_2$ with toolkit driver version $N_2$ and $M \geq N_2$ (<strong>the minor version compatibility</strong>).</li></ol></li></ol><p class="par">Generally, validating the above conditions should help whenever you run in any compatibility problems.</p><h1 id="back-to-our-problem">Back to Our Problem</h1><p>So, what’s wrong with the docker image <code>bad</code>? With above rules in hands we can perform a simple analysis.</p><p>Could it be a toolkit-driver incompatibility? Probably NO. According to Table. 1 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#default-to-minor-version">here</a>, the minor version compatibility applies with CUDA 11.x and driver &gt;= 450.80.02, which our driver version 460 satisfies, let alone binary compiled by CUDA 11.7 working like a charm in the case of docker image <code>good</code>.</p><p>It should be due to a KMD-UMD incompatibility, namely, the version of <code>libcuda.so</code> and <code>nvidia.ko</code> is incompatible. Since forward compatibility is not applicable for RTX 3090, we are expecting condition 1.2 holds, where <code>libcuda.so</code> and <code>nvidia.ko</code> should have the same version – this obviously was not the case.</p><h2 id="how-nvidia-driver-works-with-docker">How nvidia driver works with docker?</h2><p>A process in a container is technically a special process on the host, which shares the same model as other processes do to interact with GPU drivers. Since KMD runs in kernel and not interfered by user space, all programs regardless of on host or in containers are communicate with the same KMD.</p><figure class="graphviz"><svg xmlns="http://www.w3.org/2000/svg" width="296" height="70pt" viewBox="0 0 222.05 70.26"><g class="graph"><path fill="none" d="M0 70.26V0h222.05v70.26z"/><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M206.18-62.26h-93.9V-40h93.9z"/><text x="119.48" y="-47.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">host program</text></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M68.4-42.26H0V-20h68.4z"/><text x="7.2" y="-27.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">nvidia.ko</text></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M112.07-43.63c-10.62 1.73-21.89 3.56-32.43 5.27"/><path fill="#444" stroke="#444" d="M80.48-34.95 70.05-36.8l9.31-5.05z"/></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M214.05-22.26H104.4V0h109.65z"/><text x="111.6" y="-7.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">docker program</text></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M103.91-19.96c-8.05-1.31-16.26-2.64-24.07-3.91"/><path fill="#444" stroke="#444" d="m79.53-20.38-9.31-5.06 10.44-1.85z"/></g></g></svg></figure><p class="nomargin">By contrast, a program can flexibly choose which user-mode driver to link against. It can either link to the UMD installed along with the KMD on the host, or brings its own UMD during packaging and distribution.</p><figure class="graphviz"><svg xmlns="http://www.w3.org/2000/svg" width="344pt" height="70pt" viewBox="0 0 344.45 70.26"><g class="graph"><path fill="none" d="M0 70.26V0h344.45v70.26z"/><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M328.57-62.26h-93.89V-40h93.89z"/><text x="241.88" y="-47.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">host program</text></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M190.8-62.26h-86.4V-40h86.4z"/><text x="111.6" y="-47.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">libcuda.so.X</text></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M234.45-51.13h-32.13"/><path fill="#444" stroke="#444" d="m202.51-47.63-10-3.5 10-3.5z"/></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M68.4-42.26H0V-20h68.4z"/><text x="7.2" y="-27.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">nvidia.ko</text></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M104.17-43.51c-7.85 1.41-16.08 2.88-24 4.31"/><path fill="#444" stroke="#444" d="m80.81-35.76-10.46-1.68 9.22-5.21z"/></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M336.45-22.26H226.8V0h109.65z"/><text x="234" y="-7.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">docker program</text></g><g class="node" transform="translate(4 66.26)"><path fill="#fff5ee" stroke="#444" d="M190.8-22.26h-86.4V0h86.4z"/><text x="111.6" y="-7.08" fill="#444" font-family="Times New Roman,serif" font-size="14" font-weight="bold">libcuda.so.Y</text></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M226.54-11.13h-24.06"/><path fill="#444" stroke="#444" d="m202.6-7.63-10-3.5 10-3.5z"/></g><g class="edge" transform="translate(4 66.26)"><path fill="none" stroke="#444" d="M104.17-18.75c-7.85-1.41-16.08-2.88-24-4.31"/><path fill="#444" stroke="#444" d="m79.57-19.61-9.22-5.21 10.46-1.68z"/></g></g></svg></figure><p class="nomargin">We can list out all the UMDs in a running <code>good</code> container with the command:</p><div class="gk-code hljs" data-gk-id="BLOCK7"><div class="gk-code-display"><pre><span class="line"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run --gpus=<span class="hljs-string">&#x27;&quot;device=0&quot;&#x27;</span> --<span class="hljs-built_in">rm</span> -it --entrypoint= good bash</span></span><br><span class="line">root@3a19f802a459:/workspace# find / -name &#x27;libcuda.so*&#x27; -exec bash -c &quot;echo {} -\&gt; \`readlink {}\`&quot; \; 2&gt;/dev/null</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so.1 -&gt; libcuda.so.460.67</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so -&gt; libcuda.so.1</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so.460.67 -&gt;</span><br></pre></div></div><p class="nomargin">Looks like there is only one copy of <code>libcuda.so</code> that lies in <code>/usr/lib/x86_64-linux-gnu/</code> with version 460.67. However, such <code>libcuda.so</code> was not packed with the docker image from the beginning. The library disappears if you omit the <code>--gpus</code> argument:</p><div class="gk-code hljs" data-gk-id="BLOCK8"><div class="gk-code-display"><pre><span class="line"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run --<span class="hljs-built_in">rm</span> -it --entrypoint= good bash</span></span><br><span class="line">root@3a19f802a459:/workspace# find / -name &#x27;libcuda.so*&#x27; -exec bash -c &quot;echo {} -\&gt; \`readlink {}\`&quot; \; 2&gt;/dev/null</span><br><span class="line">root@3a19f802a459:/workspace#</span><br></pre></div></div><p class="par">In fact, the library exists on the host and is injected into the container by docker runtime during the startup. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361545761">This post</a> demonstrates the injection process by viewing docker’s log. Mounting <code>libcuda.so</code> from the host will maximally ensures the KMD-UMD correspondence aligned.</p><p>Now that the docker runtime would choose a native UMD, why did the image <code>bad</code> fail?</p><h2 id="the-internal-of-image-bad">The internal of image <code>bad</code></h2><p>We can likewise check the UMDs in a running <code>bad</code> container as belows:</p><div class="gk-code hljs" data-gk-id="BLOCK9"><div class="gk-code-display"><pre><span class="line"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run --gpus=<span class="hljs-string">&#x27;&quot;device=0&quot;&#x27;</span> --<span class="hljs-built_in">rm</span> -it --entrypoint= bad bash</span></span><br><span class="line">root@15f9b3c915b8:/# find / -name &#x27;libcuda.so*&#x27; -exec bash -c &quot;echo {} -\&gt; \`readlink {}\`&quot; \; 2&gt;/dev/null</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so.465.19.01 -&gt;</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so.1 -&gt; libcuda.so.465.19.01</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so -&gt; libcuda.so.1</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libcuda.so.460.67 -&gt;</span><br><span class="line">/usr/local/cuda-11.3/compat/libcuda.so.465.19.01 -&gt;</span><br><span class="line">/usr/local/cuda-11.3/compat/libcuda.so.1 -&gt; libcuda.so.465.19.01</span><br><span class="line">/usr/local/cuda-11.3/compat/libcuda.so -&gt; libcuda.so.1</span><br><span class="line">/usr/local/cuda-11.3/targets/x86_64-linux/lib/stubs/libcuda.so -&gt;</span><br></pre></div></div><p class="par">OOPS!!! Looks like there’s big difference here. We could derive two observations from the result:</p><ol><li>There is already a <code>libcuda.so</code> bundled inside the image at <code>/usr/local/cuda-11.3/compat/libcuda.so.465.19.01</code>, with a higher version of <code>465.19.01</code>.</li><li>During startup, both the native <code>libcuda.so.460.67</code> and the bundled <code>libcuda.so.465.19.01</code> are symlinked under <code>/usr/lib/x86_64-linux-gnu/</code>, and most importantly, <strong>it’s the bundled one being linked as <code>libcuda.so</code> and chosen by the program</strong>.</li></ol><p class="par"><strong>And that is the reason why the docker image <code>bad</code> violates KMD-UMD compatibility!</strong></p><h1 id="the-bug-of-libnvidia-container">The bug of libnvidia-container</h1><p>Such misbehavior is a consequence of a bug of <code>libnvidia-container</code>. But before we talk about it, let’s take a step back to see what the directory <code>/usr/local/cuda-X/compat</code> does and why should it exist.</p><p>Actually the <code>compat</code> directory is part of the CUDA compat package, according to the <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#installing-title">official docs</a>, which exists to support the forward compatibility <sup id="fnref:7"><a href="#fn:7">7</a></sup>. The official base image <code>nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04</code> had this package built in, which contains a higher version UMD <code>libcuda.so.465.19.01</code> in case of an older-versioned KMD running on the host. As aforementioned, to apply forward compatibility there exists requirement on the underlying hardware. When the requirement unsatisfied, such as for our RTX 3090 GPUs, the <code>libcuda.so</code> from compat package should hopefully not be linked against.</p><p>Unfortunately, current release of nvidia-docker <strong>would roughly attempt to apply forward compatibility, regardless of whether the GPUs meet the limitation</strong>.</p><p>The problem was encountered and studied by Gemfield who posted an article <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361545761">PyTorch 的 CUDA 错误：Error 804: forward compatibility was attempted on non supported HW</a> as explanation. Gemfield observed nvidia-docker would simultaneously symlink both the native UMD on host and the compat UMD in docker image under <code>/usr/lib/x86_64-linux-gnu/</code>, and brutely choose the one with higher version as the <code>libcuda.so.1</code>, against which user programs would link.</p><p>Obviously this behavior is neither in line with forward compatibility nor with minor version compatibility. Gemfield opened an issue <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/issues/1515">NVIDIA/nvidia-docker#1515</a> for discussion, where the author <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/issues/1515#issuecomment-872975880">guessed</a> it was a bug of libnvidia-container and another issue <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/libnvidia-container/issues/138">NVIDIA/libnvidia-container#138</a> was referred. Both issues are not yet resolved up till now.</p><p>The workaround is simple – if there’s no compat package, the compat UMD won’t be applied. We can either remove the compat package or brutely delete the <code>/usr/local/cuda-X/compat</code> directory to let it work:</p><div class="gk-unified-code diff"><div class="gk-code hljs" data-gk-id="DIFF1" data-gk-title="DIFF"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.3</span>.<span class="hljs-number">0</span>-cudnn8-devel-ubuntu20.<span class="hljs-number">04</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt update -y &amp;&amp; apt install -y python3 python3-pip</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</span></span><br><span class="gk-section gk-diff-add" data-gk-sid="DIFF1.SEC1" data-gk-type="diff-add"><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt purge cuda-compat-11-3 -y</span></span><br><span class="line"><span class="hljs-comment"># OR</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">rm</span> -rfv /usr/local/cuda-11.3/compat/</span></span><br></span><span class="line"><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;import torch; print(torch.rand(2, 3).cuda())&quot;</span>]</span></span><br></pre></div></div><div class="gk-code hljs" data-gk-id="dockerfile_bad_old:1" data-gk-title="Dockerfile_bad"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.3</span>.<span class="hljs-number">0</span>-cudnn8-devel-ubuntu20.<span class="hljs-number">04</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt update -y &amp;&amp; apt install -y python3 python3-pip</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</span></span><br><span class="line"><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;import torch; print(torch.rand(2, 3).cuda())&quot;</span>]</span></span><br></pre></div></div><div class="gk-code hljs" data-gk-id="BLOCK10" data-gk-title="Dockerfile_bad (new)"><div class="gk-code-display"><pre><span class="line"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.3</span>.<span class="hljs-number">0</span>-cudnn8-devel-ubuntu20.<span class="hljs-number">04</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt update -y &amp;&amp; apt install -y python3 python3-pip</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> apt purge cuda-compat-11-3 -y</span></span><br><span class="line"><span class="hljs-comment"># OR</span></span><br><span class="line"><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">rm</span> -rfv /usr/local/cuda-11.3/compat/</span></span><br><span class="line"><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;import torch; print(torch.rand(2, 3).cuda())&quot;</span>]</span></span><br></pre></div></div></div><div class="gk-code hljs" data-gk-id="BLOCK11"><div class="gk-code-display"><pre><span class="line">$ make bad</span><br><span class="line">tensor([[0.0059, 0.6425, 0.2299],</span><br><span class="line">        [0.2306, 0.5954, 0.0226]], device=&#x27;cuda:0&#x27;)</span><br></pre></div></div><h1 id="epilogue-2">Epilogue</h1><p>This article elaborates the cause and workaround of CUDA Error 804 when NVIDIA GPUs working with docker. As preknowledge, I introduced the consistution of CUDA, the various categories of CUDA compatibility policies, and how the docker runtime deals with GPU driver. The culprit was discovered to be a bug or deficiency of libnvidia-container, which mishandled forward compatibility and minor version compatibility and was not yet resolved. As a workaround, one can remove the CUDA compat image inside the image to avoid forward compatibility being applied and light the minor version compatibility.</p><h1 id="references-9">References</h1><ul><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361545761">PyTorch 的 CUDA 错误：Error 804: forward compatibility was attempted on non supported HW</a></li><li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/">CUDA Compatibility</a></li><li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4">Table 3. CUDA Toolkit and Corresponding Driver Versions</a></li><li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-runtime-api/driver-vs-runtime-api.html#driver-vs-runtime-api">Difference between the driver and runtime APIs</a></li><li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-drivers">CUDA Driver Lifecycles</a></li><li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/issues/1515">The latest nvidia-container-toolkit caused inconsistent cuda version and 804 error. #1515</a></li><li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/libnvidia-container/issues/138">Invalid libcuda.so.1 symlink under CUDA Enhanced Compatibility #138</a></li></ul><div class="footnotes"><ol><li class="footnote" id="fn:1"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4</a></li><li class="footnote" id="fn:2"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility">https://docs.nvidia.com/deploy/cuda-compatibility</a></li><li class="footnote" id="fn:3"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4</a></li><li class="footnote" id="fn:4">Sometimes called “Enhanced Compatibility” in old context.</li><li class="footnote" id="fn:5">To be accurate, with <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#application-considerations">some limitations</a> but should be ignorable for machine learning scenario.</li><li class="footnote" id="fn:6"><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/#forward-compatible-upgrade:~:text=Forward%20Compatibility%20is%20applicable%20only%20for%20systems%20with%20NVIDIA%20Data%20Center%20GPUs%20or%20select%20NGC%20Server%20Ready%20SKUs%20of%20RTX%20cards">https://docs.nvidia.com/deploy/cuda-compatibility/#forward-compatible-upgrade:~:text=Forward%20Compatibility%20is%20applicable%20only%20for%20systems%20with%20NVIDIA%20Data%20Center%20GPUs%20or%20select%20NGC%20Server%20Ready%20SKUs%20of%20RTX%20cards</a>.</li><li class="footnote" id="fn:7">RECALL: it’s part of UMD-KMD compatibility</li></ol></div><br><blockquote><p class="cc"><b>Author:</b> hsfzxjy.<br><b>Link:</b> <span class="cc-link"></span>.<br><b>License:</b> <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a>.<br>All rights reserved by the author.<br>Commercial use of this post in any form is <b>NOT</b> permitted.<br>Non-commercial use of this post should be attributed with this block of text.</p></blockquote><script>!function(){var n=document.querySelector("span.cc-link");n&&(n.innerHTML='<a href="'+location.href+'">'+location.href+"</a>")}()</script></div><div class="post__tags"><a href="/tags/CUDA/">CUDA</a><a href="/tags/Docker/">Docker</a></div><div class="post-nav"><a href="/disambiguate-feudalism/" class="pre">«辩义“封建”</a><a href="/gpg-and-github-interoperation/" class="next">Modern Cryptography, GPG and Integration with Git(hub)»</a></div><div id="disqus_thread"><div id="no-comment"><h2>OOPS!</h2><span>A comment box should be right here...</span><span>But it was gone due to network issues :-(</span><span>If you want to leave comments, make sure you have access to&nbsp;<a target="_blank" rel="noopener" href="https://disqus.com">disqus.com</a>.</span></div><script>var disqus_shortname="hsfzxjy",disqus_identifier="diving-from-the-cuda-error-804-into-a-bug-of-libnvidia-container/",disqus_title="Diving from the CUDA Error 804 into a bug of libnvidia-container",disqus_url="https://i.hsfzxjy.site/diving-from-the-cuda-error-804-into-a-bug-of-libnvidia-container/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.defer=!0,e.src="https://"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("body")[0]||document.getElementsByTagName("head")[0]).appendChild(e),e.onerror=function(){document.getElementById("no-comment").classList.add("show")}}()</script><script id="dsq-count-scr" src="//hsfzxjy.disqus.com/count.js" async defer></script></div></div></main><section class="aside__group"><span class="aside__btn"><div lang="zh" class="github-btn"><a href="#" rel="noopener noreferrer" target="_blank" class="gh-btn"><span class="gh-ico"></span><span class="gh-text">FOLLOW ME</span></a><a href="#" rel="noopener noreferrer" target="_blank" class="gh-count"></a></div><script>window.GH_BUTTON={username:"hsfzxjy"}</script></span><aside class="aside__left"><div class="aside__menuList"><span class="aside__menuList-item__icon"><i class="icon-home"></i></span><a href="/" class="aside__menuList-item"><span lang="zh" class="font__ui">首页 / Home</span></a><span class="aside__menuList-item__icon current"><i class="icon-embed2"></i></span><a href="/categories/Tech/" class="aside__menuList-item current"><span lang="zh" class="font__ui">科技 / Tech</span></a><span class="aside__menuList-item__icon"><i class="icon-android"></i></span><a href="/categories/Soliloquy/" class="aside__menuList-item"><span lang="zh" class="font__ui">呓语 / Soliloquy</span></a><span class="aside__menuList-item__icon"><i class="icon-leaf"></i></span><a href="/categories/Life/" class="aside__menuList-item"><span lang="zh" class="font__ui">生活 / Life</span></a><span class="aside__menuList-item__icon"><i class="icon-bookmarks"></i></span><a href="/categories/Memo/" class="aside__menuList-item"><span lang="zh" class="font__ui">速记 / Memo</span></a><span class="aside__menuList-item__icon"><i class="icon-books"></i></span><a href="/categories/Series/" class="aside__menuList-item"><span lang="zh" class="font__ui">连载 / Series</span></a><span class="aside__menuList-item__icon"><i class="icon-sigma"></i></span><a href="/works/" class="aside__menuList-item"><span lang="zh" class="font__ui">项目 / Projects</span></a><span class="aside__menuList-item__icon"><i class="icon-earth"></i></span><a href="/links/" class="aside__menuList-item"><span lang="zh" class="font__ui">友链 / Links</span></a><span class="aside__menuList-item__icon"><i class="icon-wink"></i></span><a href="/about/" class="aside__menuList-item"><span lang="zh" class="font__ui">关于 / About</span></a><span class="aside__menuList-item__icon"><i class="icon-history"></i></span><a href="/aggr/" class="aside__menuList-item"><span lang="zh" class="font__ui">索引 / Index</span></a><span class="aside__menuList-item__icon"><i class="icon-rss2"></i></span><a href="/rss.xml" class="aside__menuList-item"><span lang="zh" class="font__ui">订阅 / RSS</span></a></div></aside><aside class="aside__right"><div id="toc" lang="en" class="font__body"><div class="toc-toggler"></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#reproduction-samples"><span class="toc-number">1.</span> <span class="toc-text">Reproduction Samples</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#components-of-cuda"><span class="toc-number">2.</span> <span class="toc-text">Components of CUDA</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cuda-compatibility-policies"><span class="toc-number">3.</span> <span class="toc-text">CUDA Compatibility Policies</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#toolkit-driver-compatibility"><span class="toc-number">3.1.</span> <span class="toc-text">Toolkit-driver compatibility</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#umd-kmd-compatibility"><span class="toc-number">3.2.</span> <span class="toc-text">UMD-KMD compatibility</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary-of-compatibility"><span class="toc-number">3.3.</span> <span class="toc-text">Summary of Compatibility</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#back-to-our-problem"><span class="toc-number">4.</span> <span class="toc-text">Back to Our Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#how-nvidia-driver-works-with-docker"><span class="toc-number">4.1.</span> <span class="toc-text">How nvidia driver works with docker?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-internal-of-image-bad"><span class="toc-number">4.2.</span> <span class="toc-text">The internal of image bad</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#the-bug-of-libnvidia-container"><span class="toc-number">5.</span> <span class="toc-text">The bug of libnvidia-container</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#epilogue-2"><span class="toc-number">6.</span> <span class="toc-text">Epilogue</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#references-9"><span class="toc-number">7.</span> <span class="toc-text">References</span></a></li></ol></div></div></aside></section><div id="footer" style="text-align:center" lang="zh" class="font__ui">© <a href="/" rel="nofollow">hsfzxjy 的博客.</a>&nbsp;Powered by&nbsp;<a rel="nofollow" target="_blank" href="https://hexo.io">Hexo.</a>&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy/byak-hexo">Theme</a>&nbsp;by&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy">hsfzxjy</a>.<div style="margin-top:10px"><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011202001249" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/img/beian.png" style="float:left" loading="lazy"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤公网安备44011202001249号</span></a><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px" href="http://beian.miit.gov.cn/"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备2020075702号-1</span></a></div></div></body><script src="/dist/js/main.js" async defer></script></html>