<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=5" name="viewport"><meta content="yes" name="mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><title>Information Theory: Entropy and Mutual Information</title><meta itemprop="title" content="Information Theory: Entropy and Mutual Information - hsfzxjy 的博客"><meta itemprop="og:title" content="Information Theory: Entropy and Mutual Information - hsfzxjy 的博客"><meta itemprop="image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="og:image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="description" content="Given a discrete r.v. $X$, where $X$ ranged in $\{a_1, \l..."><meta itemprop="og:type" content="website"><link rel="stylesheet" href="/dist/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head><body><nav class="nav"><span class="nav__toggler"><span class="nav__toggler-bar bar1"></span><span class="nav__toggler-bar bar2"></span><span class="nav__toggler-bar bar3"></span></span><span class="nav__core"><div class="nav__logo"><img src="/avatar.webp" loading="lazy"></div><a href="/" lang="zh" class="nav__brand">hsfzxjy</a></span><span class="nav__togglerClose"><span class="nav__togglerClose-circle"></span><span class="nav__togglerClose-bar bar1"></span><span class="nav__togglerClose-bar bar2"></span></span><div class="nav__pathIndicator"><a href="/categories/Tech/">Tech</a></div></nav><main class="mainContainer"><div lang="en" class="post post-page"><h1 lang="en" class="post__title">Information Theory: Entropy and Mutual Information</h1><div class="post__meta font__ui">2020-01-04 | <span class="post__meta-categories"><a href="/categories/Tech/">Tech</a></span></div><div class="post__content font__body"><p>Given a discrete r.v. $X$, where $X$ ranged in $\{a_1, \ldots, a_n\}$, $\mathbb{P}(X=a_k)=p_k$. Entropy $H(X)$ is defined as:</p><p>$$H(X)= - \sum_k p_k \log p_k$$</p><p class="par">When regarded as a function of $\{p_k\}$, entropy satisfies the following properties:</p><ol><li>$H(p_1,\ldots,p_n)$ is continuous, and non-negative;</li><li>$H(p_1,\ldots,p_n)$ is convex w.r.t. $(p_1,\ldots,p_n)$;</li><li>$H(p_1,\ldots,p_n)$ has a unique maxima $(\frac{1}{n},\ldots,\frac{1}{n})$;</li><li>$H(n):=H(\frac{1}{n},\ldots,\frac{1}{n})$ increases along with $n$;</li><li>$H(p_1,\ldots,p_n)=H(p_1+\ldots+p_k,p_{k+1},\ldots,p_n)+(p_1+\ldots+p_k)H(p_{k+1}&#39;,\ldots,p_n&#39;)$.</li></ol><p class="par">Property 5 is so-called addictivity. That is, if we observe $X$ in two steps, firstly obtaining a value from $\{\hat{a},a_{k+1},\ldots,a_n\}$ and then another value from $\{a_1,\ldots,a_k\}$ if $\hat{a}$ selected, the entropy of the whole system should be sum of these two subsystems.</p><p class="nomargin">Note that a function satisfying property 1, 4, 5 must have a form of $H(\vec{p})= - C \sum_k p_k \log p_k$, which reveals that entropy function is unique.</p><p>Entropy measures the <strong>uncertainty</strong> of a random value. Intuitively, entropy reaches its maximum $\log n$ when all alphabets occur with same probability, and likewise has a minimum of $0$ if $p_k=1$ for some $k$.</p><p>Entropy also represents the smallest average length to encode a message. Say we have a message consisting of alphabets $a_1,\ldots,a_n$, occurring with probability $p_1,\ldots,p_n$. Now we want to assign a code (an $N$-ary string) to each alphabet, with no two codes sharing a same prefix. The length of the codes are denoted as $l_1,\ldots,l_n$. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">Shannon’s source coding theroem</a> states that the average code length $\sum_k p_k l_k$ could not be less than $H(p_1,\ldots,p_n)$ (taking $N$ as logarithm base).</p><p>We may generalize the definition to 2D random variables likewisely. Suppose $\mathbb{P}((X,Y)=(a_k,b_j))$ $=p(a_k,b_j)$. We define: $$H(XY)=\sum_{k,j} - p(a_k,b_j) \log p(a_k,b_j)$$. Rewrite the formula as:</p><p>$$ \begin{align} H(XY) &amp;= \sum_{k,j} - p(a_k,b_j) \log p(a_k,b_j) \\ &amp;= \sum_k\sum_j - p(a_k,b_j) \log \left[ p(b_j) p(a_k|b_j) \right] \\ &amp;= \sum_j -p(b_j) \log p(b_j) - \sum_j p(b_j) \sum_k p(a_k|b_j) \log p(a_k|b_j) \\ &amp;= H(Y) - \sum_j p(b_j) H(X|b_j) \\ &amp;= H(Y) + H(X|Y) \\ \end{align}$$</p><p class="par">where $H(X|Y)$ is weighted average of $H(X|\cdot)$, called <strong>conditional entropy</strong>. Symmetrically, we have $H(X)+H(Y|X)=H(Y)+H(X|Y)$. Further we have $H(XY) \leq H(X)+H(Y)$, since:</p><p>$$ \begin{align} H(XY)-H(X)-H(Y) &amp;= \sum_{k,j} p(a_k,b_j) \log \frac{p(a_k)p(b_j)}{p(a_k,b_j)} \\ &amp;\leq \sum_{k,j} p(a_k,b_j) \left[ \frac{p(a_k)p(b_j)}{p(a_k,b_j)} - 1 \right] \\ &amp;=0 \end{align}$$</p><p class="par">the equality holds $\iff X, Y$ are independent.</p><p>In summary, we have:</p><p>$$\begin{align} H(XY) &amp;\leq H(X)+H(Y) \\ H(Y|X) &amp;\leq H(Y) \\ H(X|Y) &amp;\leq H(X) \\ \end{align}$$</p><p class="par">The latter two are intuitive – entropy (uncertainty) decreases if provided some prior.</p><h2 id="discrete-mutual-information">Discrete Mutual Information</h2><p class="par">The mutual information of $X,Y$ is defined as:</p><p>$$\begin{align} I(X|Y) &amp;= H(X)-H(X|Y) \\ &amp;= - \sum_k p(a_k) \log p(a_k) + \sum_j \sum_k p(a_k,b_j) \log p(a_k|b_j) \\ &amp;= \sum_{j,k} p(a_k,b_j) \log \frac{p(a_k, b_j)}{p(a_k)p(b_j)} \end{align}$$</p><p class="par">Intuitively, $I(X;Y)$ is the reduced information after prior $Y$ provided. It’s worth noting that $I(X;Y)=I(Y;X)$ and thus $I(\cdot;\cdot)$ is symmetric. We can define conditional mutual information likewisely: $$\begin{align} I(X;Y|Z) &amp;= \sum_i p(z_i) I(X|z_i;Y|z_i) \\ &amp;= \sum_i p(z_i) \sum_{k,j} p(a_k,b_j|z_i) \log \frac{p(a_k, b_j|z_i)}{p(a_k|z_i)p(b_j|z_i)} \\ &amp;= \sum_{i,j,k} p(a_k,b_j,z_i) \log \frac{p(a_k, b_j,z_i)p(z_i)}{p(a_k,z_i)p(b_j,z_i)} \end{align}$$</p><p>Some facts can be derived using the definition of entropy and mutual information:</p><p>$$\begin{align} I(X;Y) &amp;= H(X) - H(X|Y) \\ &amp;= H(X) + H(Y) - H(XY) \\ I(X;YZ) &amp;= H(X) + H(YZ) - H(XYZ) \\ I(X;Y|Z) &amp;= H(X|Z) - H(X|YZ) \\ &amp;= H(XZ) + H(YZ) - H(Z) - H(XYZ) \end{align}$$</p><p class="par">We can further define mutual information among three (or more) variables as: $$\begin{align} I(X;Y;Z) &amp;= I(X;Y)-I(X;Y|Z) \\ &amp;= H(X)+H(Y)+H(Z)-H(XY)-H(YZ)-H(XZ)+H(XYZ) \\ \end{align}$$</p><p class="par">We may observe some similarity between the above formula and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle">Inclusion-Exclusion Principle</a>. In fact, we can build a formal correspondence between entropy and set measure, as:</p><p>$$\begin{matrix} &amp;H(X)=\mu(A) &amp;H(XY)=\mu(A \cup B) \\ &amp;H(X|Y)=\mu(A-B) &amp;I(X;Y)=\mu(A \cap B) \\ &amp;H(X;Y|Z)=\mu((A \cap B)-C) &amp;I(X;Y;Z)=\mu(A \cap B \cap C) \end{matrix}$$</p><p class="par">where $\mu$ is some set measure. The correspondence may be used as a helper for memorizing.</p><h2 id="continuous-cases">Continuous Cases</h2><p class="par">We first try to generalize entropy to continuous r.v.. Suppose $X$ has pdf $p(x)$, and $\pi=\{x_i\}_{-\infty}^\infty$ is a partition of $\mathbb{R}$, $|\pi|=\Delta x$. We may resemble discrete cases and write down the formula of “entropy”:</p><p>$$\sum_i - p(x_i) \log (p(x_i) \Delta x)$$</p><p class="par">Taking $\Delta x \rightarrow 0$, we have:</p><p>$$\begin{align} &amp;\lim_{\Delta x \rightarrow 0}\sum_i - p(x_i) \log (p(x_i) \Delta x) \\ =&amp;\int -p(x) \log p(x) dx - \lim_{\Delta x \rightarrow 0}\sum_i - p(x_i) \log \Delta x \end{align}$$</p><p class="par">In practice, we omit the second term (since it’s infinite) and define the first one as <strong>differential entropy</strong>:</p><p>$$h(X):=\int_\mathbb{R} -p(x) \log p(x) dx$$</p><p class="par">We can define $h(X|Y)$ and $I(X;Y)$ likewisely.</p><br><blockquote><p class="cc"><b>Author:</b> hsfzxjy.<br><b>Link:</b> <span class="cc-link"></span>.<br><b>License:</b> <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a>.<br>All rights reserved by the author.<br>Commercial use of this post in any form is <b>NOT</b> permitted.<br>Non-commercial use of this post should be attributed with this block of text.</p></blockquote><script>!function(){var n=document.querySelector("span.cc-link");n&&(n.innerHTML='<a href="'+location.href+'">'+location.href+"</a>")}()</script></div><div class="post__tags"><a href="/tags/Information-Theory/">Information Theory</a></div><div class="post-nav"><a href="/information-theory-kl-divergence/" class="pre">«Information Theory: KL Divergence</a><a href="/yakimono/" class="next">铁板烧»</a></div><div id="disqus_thread"><div id="no-comment"><h2>OOPS!</h2><span>A comment box should be right here...</span><span>But it was gone due to network issues :-(</span><span>If you want to leave comments, make sure you have access to&nbsp;<a target="_blank" rel="noopener" href="https://disqus.com">disqus.com</a>.</span></div><script>var disqus_shortname="hsfzxjy",disqus_identifier="information-theory-entropy-and-mutual-information/",disqus_title="Information Theory: Entropy and Mutual Information",disqus_url="https://i.hsfzxjy.site/information-theory-entropy-and-mutual-information/";!function(){var t=document.createElement("script");t.type="text/javascript",t.async=!0,t.defer=!0,t.src="https://"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("body")[0]||document.getElementsByTagName("head")[0]).appendChild(t),t.onerror=function(){document.getElementById("no-comment").classList.add("show")}}()</script><script id="dsq-count-scr" src="//hsfzxjy.disqus.com/count.js" async defer></script></div></div></main><section class="aside__group"><span class="aside__btn"><div lang="zh" class="github-btn"><a href="#" rel="noopener noreferrer" target="_blank" class="gh-btn"><span class="gh-ico"></span><span class="gh-text">FOLLOW ME</span></a><a href="#" rel="noopener noreferrer" target="_blank" class="gh-count"></a></div><script>window.GH_BUTTON={username:"hsfzxjy"}</script></span><aside class="aside__left"><div class="aside__menuList"><span class="aside__menuList-item__icon"><i class="icon-home"></i></span><a href="/" class="aside__menuList-item"><span lang="zh" class="font__ui">首页 / Home</span></a><span class="aside__menuList-item__icon current"><i class="icon-embed2"></i></span><a href="/categories/Tech/" class="aside__menuList-item current"><span lang="zh" class="font__ui">科技 / Tech</span></a><span class="aside__menuList-item__icon"><i class="icon-android"></i></span><a href="/categories/Soliloquy/" class="aside__menuList-item"><span lang="zh" class="font__ui">呓语 / Soliloquy</span></a><span class="aside__menuList-item__icon"><i class="icon-leaf"></i></span><a href="/categories/Life/" class="aside__menuList-item"><span lang="zh" class="font__ui">生活 / Life</span></a><span class="aside__menuList-item__icon"><i class="icon-bookmarks"></i></span><a href="/categories/Memo/" class="aside__menuList-item"><span lang="zh" class="font__ui">速记 / Memo</span></a><span class="aside__menuList-item__icon"><i class="icon-books"></i></span><a href="/categories/Series/" class="aside__menuList-item"><span lang="zh" class="font__ui">连载 / Series</span></a><span class="aside__menuList-item__icon"><i class="icon-sigma"></i></span><a href="/works/" class="aside__menuList-item"><span lang="zh" class="font__ui">项目 / Projects</span></a><span class="aside__menuList-item__icon"><i class="icon-earth"></i></span><a href="/links/" class="aside__menuList-item"><span lang="zh" class="font__ui">友链 / Links</span></a><span class="aside__menuList-item__icon"><i class="icon-wink"></i></span><a href="/about/" class="aside__menuList-item"><span lang="zh" class="font__ui">关于 / About</span></a><span class="aside__menuList-item__icon"><i class="icon-history"></i></span><a href="/aggr/" class="aside__menuList-item"><span lang="zh" class="font__ui">索引 / Index</span></a><span class="aside__menuList-item__icon"><i class="icon-rss2"></i></span><a href="/rss.xml" class="aside__menuList-item"><span lang="zh" class="font__ui">订阅 / RSS</span></a></div></aside><aside class="aside__right"><div id="toc" lang="en" class="font__body"><div class="toc-toggler"></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#discrete-mutual-information"><span class="toc-number">1.</span> <span class="toc-text">Discrete Mutual Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#continuous-cases"><span class="toc-number">2.</span> <span class="toc-text">Continuous Cases</span></a></li></ol></div></div></aside></section><div id="footer" style="text-align:center" lang="zh" class="font__ui">© <a href="/" rel="nofollow">hsfzxjy 的博客.</a>&nbsp;Powered by&nbsp;<a rel="nofollow" target="_blank" href="https://hexo.io">Hexo.</a>&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy/byak-hexo">Theme</a>&nbsp;by&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy">hsfzxjy</a>.<div style="margin-top:10px"><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011202001249" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/img/beian.png" style="float:left" loading="lazy"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤公网安备44011202001249号</span></a><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px" href="http://beian.miit.gov.cn/"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备2020075702号-1</span></a></div></div></body><script src="/dist/js/main.js" async defer></script></html>