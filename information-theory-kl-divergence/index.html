<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=5" name="viewport"><meta content="yes" name="mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><script>(()=>{let e="/cnfonts.js?rDuukLHQ",t=document,n=navigator.serviceWorker,r=!1;var a;(r=n&&(n.register(e,{scope:"/"}).then(e=>e.update()),a=n.controller)&&a.scriptURL.endsWith(e)&&"activated"===a.state?!0:r)?((a=t.createElement("link")).rel="stylesheet",a.href="/fontchan/rDuukLHQ.css",a.blocking="render",t.head.appendChild(a)):((a=t.createElement("script")).src=e,a.onload=()=>$fontchan.injectCss(),t.head.appendChild(a))})()</script><title>Information Theory: KL Divergence</title><meta itemprop="title" content="Information Theory: KL Divergence - hsfzxjy 的博客"><meta itemprop="og:title" content="Information Theory: KL Divergence - hsfzxjy 的博客"><meta itemprop="image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="og:image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="description" content="Assume there are two hypotheses $H_1$ and $H_2$, r.v. $X$..."><meta itemprop="og:type" content="website"><link rel="stylesheet" href="/dist/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head><body><nav class="nav"><span class="nav__toggler"><span class="nav__toggler-bar bar1"></span><span class="nav__toggler-bar bar2"></span><span class="nav__toggler-bar bar3"></span></span><span class="nav__core"><div class="nav__logo"><img src="/avatar.webp" loading="lazy"></div><a href="/" lang="zh" class="nav__brand">hsfzxjy</a></span><span class="nav__togglerClose"><span class="nav__togglerClose-circle"></span><span class="nav__togglerClose-bar bar1"></span><span class="nav__togglerClose-bar bar2"></span></span><div class="nav__pathIndicator"><a href="/categories/Tech/">Tech</a></div></nav><main class="mainContainer"><div lang="en" class="post post-page"><h1 lang="en" class="post__title">Information Theory: KL Divergence</h1><div class="post__meta font__ui">2020-01-15 | <span class="post__meta-categories"><a href="/categories/Tech/">Tech</a></span></div><div class="post__content font__body"><p>Assume there are two hypotheses $H_1$ and $H_2$, r.v. $X$ ranged in alphabets $\{a_1,\ldots\,a_k\}$. Under hypothesis $H_i$, $X$ has pdf $p(X=a_j|H_i)=p_i(a_j)$. According to Law of Total Probability, we have:</p><p>$$ p(H_i|a_k) = \frac{p(H_i)p_i(a_k)}{p_1(a_k)p(H_1)+p_2(a_k)p(H_2)} $$</p><p class="par">The formula can be transformed into:</p><p>$$ \log \frac{p_2(a_k)}{p_1(a_k)} = \log \frac{p(H_2|a_k)}{p(H_1|a_k)} - \log \frac{p(H_2)}{p(H_1)} $$</p><p class="par">which implies that, $\log \frac{p_2(a_k)}{p_1(a_k)}$ equals the difference of log likelihood ratio before and after conditioning $X=a_k$. We define $\log \frac{p_2(a_k)}{p_1(a_k)}$ be the discrimination information for $H_2$ over $H_1$, when $X=a_k$. The expectation of discrimination information is KL divergence, denoted as:</p><p>$$D_{KL}(P_2||P_1) = \sum_k p_2(a_k) \log \frac{p_2(a_k)}{p_1(a_k)} $$</p><p class="par">which sometimes denoted as $I(p2,p1;X)$, or simply $I(p2,p1)$ if without ambiguity.</p><p>KL Divergence can be interpreted as a measure of expected information for $X$ gained after distribution shifted from $p_1$ to $p_2$, where $p_1$ and $p_2$ regarded as prior and post-prior distributions.</p><p class="par">Further we may define the (symmetrised) divergence between $p_2$ and $p_1$:</p><p>$$J(p_2,p_1;X)=D_{KL}(p_2||p_1) + D_{KL}(p_1||p_2) = J(p_1,p_2;X)$$</p><p class="par">Divergence $J(p_2,p_1)$ is a measure of difference between two distributions satisfying</p><ul><li>$J(p_1,p_2) \geq 0$ and equality holds iff $p_1=p_2$;</li><li>$J(p_1,p_2) = J(p_2,p_1)$.</li></ul><p class="par">but not triangle inequality, and thus $J(\cdot,\cdot)$ is not a distance function.</p><h2 id="generalization">Generalization</h2><p class="par">We may generalize the definition to continuous variables as:</p><p>$$D_{KL}(p_2||p_1) = \int p_2(x) \log \frac{p_2(x)}{p_1(x)} dx$$</p><p class="par">or multivariate cases:</p><p>$$D_{KL}(p_2(\vec{x})||p_1(\vec{x})) = \int p_2(\vec{x}) \log \frac{p_2(\vec{x})}{p_1(\vec{x})} d\vec{x}$$</p><p class="par">or further, conditional cases:</p><p>$$D_{KL}(p_2(X|Y)||p_1(X|Y)) = \int p_2(y) p_2(x|y) \log \frac{p_2(x|y)}{p_1(x|y)} dxdy$$</p><h2 id="relation-to-shannon-entropy">Relation to Shannon Entropy</h2><p class="par">For a discrete variable $X$, with distribution $p$, we have:</p><p>$$H(X)=\log n - D_{KL}(p||v)$$</p><p class="par">where $n$ is number of values $X$ can take on, and $v$ is the uniform distribution on those values. The lemma suggests that $D_{KL}(p||v)$ measures the information difference between $p$ and a totally chaotic “bottom distribution”. From the perspective of transmission, KL divergence equals the length of additional bits required to encode a message, when the distribution of source alphabets shifts from $p$ to $v$.</p><p>For a continuous variable $X$ ranged within a finite set of volume $L$, we have:</p><p>$$H(X) = \log L - D_{KL}(p||v)$$</p><p class="par">where $v$ is a uniform distribution over the range of $X$. The interpretation is similar to discrete cases.</p><h2 id="relation-to-mutual-information">Relation to Mutual Information</h2><p class="par">We can observe that</p><p>$$I(X;Y)=D_{KL}(P(XY)||P(X)P(Y))$$</p><p class="par">which suggests mutual information $I(X;Y)$ is the information difference when distribution of $(X,Y)$ shifted from $P(X)P(Y)$ to $P(X,Y)$, i.e., from independent to dependent.</p><h2 id="additional-properties-of-kl-divergence">Additional Properties of KL Divergence</h2><p class="noindent"><strong>Addictivity</strong> Assume $X,Y$ are independent, and $P(x,y)=P_1(x)P_2(y)$, $Q$ likewisely. We have</p><p>$$\begin{align} D_{KL}(P(X,Y)||Q(X,Y)) &amp;= \int P(x,y) \log \frac{P(x,y)}{Q(x,y)} dxdy \\ &amp;= \int P_1(x)P_2(y) \log \frac{P_1(x)P_2(y)}{Q_1(x)Q_2(y)} dxdy \\ &amp;= \int P_1(x) \log \frac{P_1(x)}{Q_1(x)} dx + \int P_2(y) \log \frac{P_2(y)}{Q_2(y)} dy \\ &amp;= D_{KL}(P_1||Q_1) + D_{KL}(P_2||Q_2) \end{align}$$</p><p class="noindent"><strong>Convexity</strong> Assume $p_1,p_2,q_1,q_2,p,q$ are distributions, $0 \leq \lambda \leq 1$. We have</p><p>$$D_{KL}(\lambda p_1 + (1 - \lambda) p_2 || q) \leq \lambda D_{KL}(p_1||q) + (1 - \lambda) D_{KL}(p_2||q)$$</p><p class="par">which can be derived from $\log x \leq x - 1$ when $0 &lt; x &lt; 1$. Similarly we have</p><p>$$D_{KL}(p || \lambda q_1 + (1 - \lambda) q_2) \leq \lambda D_{KL}(p||q_1) + (1 - \lambda) D_{KL}(p||q_2)$$</p><p class="par">which can be derived from the convexity of $\log(\cdot)$. Further we have</p><p>$$D_{KL}(\lambda p_1 + (1 - \lambda) p_2 || \lambda q_1 + (1 - \lambda) q_2) \leq \lambda D_{KL}(p_1 || q_1) + (1 - \lambda) D_{KL}(p_2|| q_2)$$</p><p class="par">which can be proved using the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Log_sum_inequality">Log Sum Inequality</a>.</p><p class="noindent"><strong>Invariance</strong> Assume $U=g(X)$, then $D_{KL}(p(X)||q(X))=D_{KL}(p(U)||q(U))$, i.e., the form of KL divergence remains invariant under transforms. Mutual information, however, does not have such property.</p><br><blockquote><p class="cc"><b>Author:</b> hsfzxjy.<br><b>Link:</b> <span class="cc-link"></span>.<br><b>License:</b> <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a>.<br>All rights reserved by the author.<br>Commercial use of this post in any form is <b>NOT</b> permitted.<br>Non-commercial use of this post should be attributed with this block of text.</p></blockquote><script>!function(){var n=document.querySelector("span.cc-link");n&&(n.innerHTML='<a href="'+location.href+'">'+location.href+"</a>")}()</script></div><div class="post__tags"><a href="/tags/Information-Theory/">Information Theory</a></div><div class="post-nav"><a href="/obtain-a-random-unused-tcp-port-with-bash/" class="pre">«Obtain a Random Available TCP Port with Bash</a><a href="/information-theory-entropy-and-mutual-information/" class="next">Information Theory: Entropy and Mutual Information»</a></div><div id="disqus_thread"><div id="no-comment"><h2>OOPS!</h2><span>A comment box should be right here...</span><span>But it was gone due to network issues :-(</span><span>If you want to leave comments, make sure you have access to&nbsp;<a target="_blank" rel="noopener" href="https://disqus.com">disqus.com</a>.</span></div><script>var disqus_shortname="hsfzxjy",disqus_identifier="information-theory-kl-divergence/",disqus_title="Information Theory: KL Divergence",disqus_url="https://i.hsfzxjy.site/information-theory-kl-divergence/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.defer=!0,e.src="https://"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("body")[0]||document.getElementsByTagName("head")[0]).appendChild(e),e.onerror=function(){document.getElementById("no-comment").classList.add("show")}}()</script><script id="dsq-count-scr" src="//hsfzxjy.disqus.com/count.js" async defer></script></div></div></main><section class="aside__group"><span class="aside__btn"><div lang="zh" class="github-btn"><a href="#" rel="noopener noreferrer" target="_blank" class="gh-btn"><span class="gh-ico"></span><span class="gh-text">FOLLOW ME</span></a><a href="#" rel="noopener noreferrer" target="_blank" class="gh-count"></a></div><script>window.GH_BUTTON={username:"hsfzxjy"}</script></span><aside class="aside__left"><div class="aside__menuList"><span class="aside__menuList-item__icon"><i class="icon-home"></i></span><a href="/" class="aside__menuList-item"><span lang="zh" class="font__ui">首页 / Home</span></a><span class="aside__menuList-item__icon current"><i class="icon-embed2"></i></span><a href="/categories/Tech/" class="aside__menuList-item current"><span lang="zh" class="font__ui">科技 / Tech</span></a><span class="aside__menuList-item__icon"><i class="icon-android"></i></span><a href="/categories/Soliloquy/" class="aside__menuList-item"><span lang="zh" class="font__ui">呓语 / Soliloquy</span></a><span class="aside__menuList-item__icon"><i class="icon-leaf"></i></span><a href="/categories/Life/" class="aside__menuList-item"><span lang="zh" class="font__ui">生活 / Life</span></a><span class="aside__menuList-item__icon"><i class="icon-bookmarks"></i></span><a href="/categories/Memo/" class="aside__menuList-item"><span lang="zh" class="font__ui">速记 / Memo</span></a><span class="aside__menuList-item__icon"><i class="icon-books"></i></span><a href="/categories/Series/" class="aside__menuList-item"><span lang="zh" class="font__ui">连载 / Series</span></a><span class="aside__menuList-item__icon"><i class="icon-sigma"></i></span><a href="/works/" class="aside__menuList-item"><span lang="zh" class="font__ui">项目 / Projects</span></a><span class="aside__menuList-item__icon"><i class="icon-earth"></i></span><a href="/links/" class="aside__menuList-item"><span lang="zh" class="font__ui">友链 / Links</span></a><span class="aside__menuList-item__icon"><i class="icon-wink"></i></span><a href="/about/" class="aside__menuList-item"><span lang="zh" class="font__ui">关于 / About</span></a><span class="aside__menuList-item__icon"><i class="icon-history"></i></span><a href="/aggr/" class="aside__menuList-item"><span lang="zh" class="font__ui">索引 / Index</span></a><span class="aside__menuList-item__icon"><i class="icon-rss2"></i></span><a href="/rss.xml" class="aside__menuList-item"><span lang="zh" class="font__ui">订阅 / RSS</span></a></div></aside><aside class="aside__right"><div id="toc" lang="en" class="font__body"><div class="toc-toggler"></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#generalization"><span class="toc-number">1.</span> <span class="toc-text">Generalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#relation-to-shannon-entropy"><span class="toc-number">2.</span> <span class="toc-text">Relation to Shannon Entropy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#relation-to-mutual-information"><span class="toc-number">3.</span> <span class="toc-text">Relation to Mutual Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#additional-properties-of-kl-divergence"><span class="toc-number">4.</span> <span class="toc-text">Additional Properties of KL Divergence</span></a></li></ol></div></div></aside></section><div id="footer" style="text-align:center" lang="zh" class="font__ui">© <a href="/" rel="nofollow">hsfzxjy 的博客.</a>&nbsp;Powered by&nbsp;<a rel="nofollow" target="_blank" href="https://hexo.io">Hexo.</a>&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy/byak-hexo">Theme</a>&nbsp;by&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy">hsfzxjy</a>.<div style="margin-top:10px"><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011202001249" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/img/beian.png" style="float:left" loading="lazy"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤公网安备44011202001249号</span></a><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px" href="http://beian.miit.gov.cn/"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备2020075702号-1</span></a></div></div></body><script src="/dist/js/main.js" async defer></script></html>