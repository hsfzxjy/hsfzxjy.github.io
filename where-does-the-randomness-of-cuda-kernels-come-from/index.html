<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=5" name="viewport"><meta content="yes" name="mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><script>(()=>{let e="/cnfonts.js?rDuukLHQ",t=document,n=navigator.serviceWorker,r=!1;var a;(r=n&&(n.register(e,{scope:"/"}).then(e=>e.update()),a=n.controller)&&a.scriptURL.endsWith(e)&&"activated"===a.state?!0:r)?((a=t.createElement("link")).rel="stylesheet",a.href="/fontchan/rDuukLHQ.css",a.blocking="render",t.head.appendChild(a)):((a=t.createElement("script")).src=e,a.onload=()=>$fontchan.injectCss(),t.head.appendChild(a))})()</script><title>Demystify the randomness in CUDA kernels</title><meta itemprop="title" content="Demystify the randomness in CUDA kernels - hsfzxjy 的博客"><meta itemprop="og:title" content="Demystify the randomness in CUDA kernels - hsfzxjy 的博客"><meta itemprop="image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="og:image" content="https://i.hsfzxjy.site/avatar-for-hsfzxjy.jpg"><meta itemprop="description" content="You might have heard that many CUDA operators contains so..."><meta itemprop="og:type" content="website"><link rel="stylesheet" href="/dist/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="alternate" type="application/rss+xml" href="/rss.xml"></head><body><nav class="nav"><span class="nav__toggler"><span class="nav__toggler-bar bar1"></span><span class="nav__toggler-bar bar2"></span><span class="nav__toggler-bar bar3"></span></span><span class="nav__core"><div class="nav__logo"><img src="/avatar.webp" loading="lazy"></div><a href="/" lang="zh" class="nav__brand">hsfzxjy</a></span><span class="nav__togglerClose"><span class="nav__togglerClose-circle"></span><span class="nav__togglerClose-bar bar1"></span><span class="nav__togglerClose-bar bar2"></span></span><div class="nav__pathIndicator"><a href="/categories/Tech/">Tech</a></div></nav><main class="mainContainer"><div lang="en" class="post post-page"><h1 lang="en" class="post__title">Demystify the randomness in CUDA kernels</h1><div class="post__meta font__ui">2021-06-11 | <span class="post__meta-categories"><a href="/categories/Tech/">Tech</a></span></div><div class="post__content font__body"><p>You might have heard that many CUDA operators contains some kind of non-determinism, and to eliminate the randomness, one must pay for the degradation of performance. The warning occurs many times in blog posts or framework documentation, but few of them give a detailed explanation for the source of randomness. To this end, the post is going to explore the problem.</p><p>When talked about GPU computation, one might come up with a notion of some super-fast hardwares. The surprising speed comes from intensive parallelism of the architecture, which allows users to run thousands of routines on parallel (compared to dozens on ordinary CPUs). The routines are called <strong>threads</strong>, and similar to the concept with the same name in operating systems, they suffer from <strong>non-deterministic execution order</strong> and <strong>data race condition</strong>.</p><p>Non-deterministic execution order means, if we arrange all instructions of different threads into a sequence, ordered by their occurrence time, the sequence could vary greatly across invocations. If two threads run on parallel, each with a single instruction, we cannot tell which one is executed first. This is the fundamental origin of randomness, and is inevitable.</p><p>Data race condition is one of the consequences of non-deterministic execution order. When the threads is manipulating some shared variables, and the manipulation is not <em>atomic</em>, i.e. consists of interruptible instruction sequence, the program might yield undesired results. Programs should be carefully designed to avoid race condition, with the help of locks or atomic operations. To alleviate, CUDA provides atomic arithmetic routines like <code>atomicAdd()</code> or <code>atomicMax()</code> for safe access to shared memory.</p><p>By far we have seen that there does exist some kind of randomness inside GPUs, and if not handled properly, our program will give incorrect results when working with shared variables. But one may argue that, we have atomic operations like <code>atomicAdd()</code>. If a program correctly sums up the same collection of numbers, although the order might be messed, it should always returns the same result. Sadly this is wrong, since some arithmetic operations <strong>DOES rely on the order of operands</strong>! Let’s take the following CUDA program as an example:</p><div class="gk-code hljs" data-gk-id="BLOCK1"><div class="gk-code-display"><pre><span class="line"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">sum</span><span class="hljs-params">(<span class="hljs-type">float</span> *result)</span> </span>{</span><br><span class="line">    <span class="hljs-type">float</span> i = (<span class="hljs-type">float</span>)blockIdx.x;</span><br><span class="line">    <span class="hljs-built_in">atomicAdd</span>(result, i / <span class="hljs-number">10</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="hljs-type">static</span> <span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>{</span><br><span class="line">    <span class="hljs-type">float</span> *cu_result;</span><br><span class="line">    <span class="hljs-built_in">cudaMalloc</span>(&amp;cu_result, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));</span><br><span class="line">    <span class="hljs-type">float</span> result;</span><br><span class="line">    <span class="hljs-type">int</span>   i;</span><br><span class="line">    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">10</span>; i++) {</span><br><span class="line">        <span class="hljs-built_in">cudaMemset</span>(cu_result, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));</span><br><span class="line">        sum&lt;&lt;&lt;N, <span class="hljs-number">1</span>&gt;&gt;&gt;(cu_result);</span><br><span class="line">        <span class="hljs-built_in">cudaMemcpy</span>(&amp;result, cu_result, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line">        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);</span><br><span class="line">    }</span><br><span class="line">    <span class="hljs-built_in">cudaFree</span>(cu_result);</span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;</span><br><span class="line">}</span><br></pre></div></div><p class="par">In this simple program, we implement a parallelized summing function with <code>atomicAdd()</code>. We sum up $\{0.0,0.1,0.2,\ldots,99.8,99.9\}$ for 10 times and print out the results. The results would look like:</p><div class="gk-code hljs" data-gk-id="BLOCK2"><div class="gk-code-display"><pre><span class="line">49949.996094</span><br><span class="line">49950.003906</span><br><span class="line">49950.000000</span><br><span class="line">49950.000000</span><br><span class="line">49949.996094</span><br><span class="line">49950.000000</span><br><span class="line">49949.988281</span><br><span class="line">49949.992188</span><br><span class="line">49949.996094</span><br><span class="line">49950.000000</span><br></pre></div></div><p class="par">which is completely non-deterministic with some random small errors (the accurate answer should be 49950).</p><p>The key is that, <strong>floating-point arithemtics are non-associative</strong>. The equation $(a + b) + c = a + (b + c)$ does NOT hold for floating-point numbers $a, b, c$.</p><p>To understand this, let’s consider an extreme example – a very simple decimal floating-point representation with no fraction digits, i.e., numbers are represented as $\pm b \times 10^{e}$, where $b \in \{0 \ldots 9\}$. We will calculate $(0.8+1.5)-1.6$ and $0.8+(1.5-1.6)$ under this kind of “specification”. Below are their representation:</p><p>$$ \begin{aligned} 0.8 &amp;\rightarrow 8 \times 10^{-1} \\ 1.5 &amp;\rightarrow 2 \times 10^{0} \\ 1.6 &amp;\rightarrow 2 \times 10^{0} \\ \end{aligned} $$</p><p class="nomargin">For $(0.8 + 1.5)-1.6$, we first shift the base of $8 \times 10^{-1}$ by 1 digit and get $0.8 \times 10^{0}$. Due to an underflow, it is truncated and becomes $0 \times 10^{0}$. Then sum up $0 \times 10^{0}$ (0.8) and $2 \times 10^{0}$ (1.5) and get $2 \times 10^{0}$. Further $2 \times 10^{0} - 2 \times 10^{0}$ (1.6), results in $0 \times 10^{0}$.</p><p class="nomargin">For $0.8 + (1.5-1.6)$, first calculate $2 \times 10^0$ (1.5) $-2 \times 10^0$ (1.6) and get $0 \times 10^{0}$. Then $8 \times 10^{-1}$ (0.8) adding with $0 \times 10^0$, results in $8 \times 10^{-1}$.</p><p>The example showcases a situation that floating-point underflow during summation could lead to extra numeric error, and further causes non-associativity in the arithemtic. It is simple, but can be generalized to the current IEEE 754 floating-point numbers. <em>The operands’ order matters in floating-point arithemtics!</em></p><p>So here’s the answer. When running parallelized routines, GPUs have some innate randomness which comes from non-deterministic execution order. If a program consists of order-dependent operations, it will give non-deterministic output. Floating-point arithemtics, used widely in CUDA algorithms, are also such kind of operations, though less known by people. Atomic arithemtics solve the race condition problem, but do not guarantee the order.</p><p>If one requires determinism, the program should sequentialize the operations by using single thread or special synchronization, which will lead to a performance degrade. There is always a trade-off between determinism and performance.</p><br><blockquote><p class="cc"><b>Author:</b> hsfzxjy.<br><b>Link:</b> <span class="cc-link"></span>.<br><b>License:</b> <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a>.<br>All rights reserved by the author.<br>Commercial use of this post in any form is <b>NOT</b> permitted.<br>Non-commercial use of this post should be attributed with this block of text.</p></blockquote><script>!function(){var n=document.querySelector("span.cc-link");n&&(n.innerHTML='<a href="'+location.href+'">'+location.href+"</a>")}()</script></div><div class="post__tags"><a href="/tags/CUDA/">CUDA</a><a href="/tags/Determinism/">Determinism</a></div><div class="post-nav"><a href="/language-ethusiast-and-pragmatist/" class="pre">«语言狂热者与实用主义者</a><a href="/indexeddb-performant-bulk-mutations/" class="next">Performant Bulk Mutations in IndexedDB»</a></div><div id="disqus_thread"><div id="no-comment"><h2>OOPS!</h2><span>A comment box should be right here...</span><span>But it was gone due to network issues :-(</span><span>If you want to leave comments, make sure you have access to&nbsp;<a target="_blank" rel="noopener" href="https://disqus.com">disqus.com</a>.</span></div><script>var disqus_shortname="hsfzxjy",disqus_identifier="where-does-the-randomness-of-cuda-kernels-come-from/",disqus_title="Demystify the randomness in CUDA kernels",disqus_url="https://i.hsfzxjy.site/where-does-the-randomness-of-cuda-kernels-come-from/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.defer=!0,e.src="https://"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("body")[0]||document.getElementsByTagName("head")[0]).appendChild(e),e.onerror=function(){document.getElementById("no-comment").classList.add("show")}}()</script><script id="dsq-count-scr" src="//hsfzxjy.disqus.com/count.js" async defer></script></div></div></main><section class="aside__group"><span class="aside__btn"><div lang="zh" class="github-btn"><a href="#" rel="noopener noreferrer" target="_blank" class="gh-btn"><span class="gh-ico"></span><span class="gh-text">FOLLOW ME</span></a><a href="#" rel="noopener noreferrer" target="_blank" class="gh-count"></a></div><script>window.GH_BUTTON={username:"hsfzxjy"}</script></span><aside class="aside__left"><div class="aside__menuList"><span class="aside__menuList-item__icon"><i class="icon-home"></i></span><a href="/" class="aside__menuList-item"><span lang="zh" class="font__ui">首页 / Home</span></a><span class="aside__menuList-item__icon current"><i class="icon-embed2"></i></span><a href="/categories/Tech/" class="aside__menuList-item current"><span lang="zh" class="font__ui">科技 / Tech</span></a><span class="aside__menuList-item__icon"><i class="icon-android"></i></span><a href="/categories/Soliloquy/" class="aside__menuList-item"><span lang="zh" class="font__ui">呓语 / Soliloquy</span></a><span class="aside__menuList-item__icon"><i class="icon-leaf"></i></span><a href="/categories/Life/" class="aside__menuList-item"><span lang="zh" class="font__ui">生活 / Life</span></a><span class="aside__menuList-item__icon"><i class="icon-bookmarks"></i></span><a href="/categories/Memo/" class="aside__menuList-item"><span lang="zh" class="font__ui">速记 / Memo</span></a><span class="aside__menuList-item__icon"><i class="icon-books"></i></span><a href="/categories/Series/" class="aside__menuList-item"><span lang="zh" class="font__ui">连载 / Series</span></a><span class="aside__menuList-item__icon"><i class="icon-sigma"></i></span><a href="/works/" class="aside__menuList-item"><span lang="zh" class="font__ui">项目 / Projects</span></a><span class="aside__menuList-item__icon"><i class="icon-earth"></i></span><a href="/links/" class="aside__menuList-item"><span lang="zh" class="font__ui">友链 / Links</span></a><span class="aside__menuList-item__icon"><i class="icon-wink"></i></span><a href="/about/" class="aside__menuList-item"><span lang="zh" class="font__ui">关于 / About</span></a><span class="aside__menuList-item__icon"><i class="icon-history"></i></span><a href="/aggr/" class="aside__menuList-item"><span lang="zh" class="font__ui">索引 / Index</span></a><span class="aside__menuList-item__icon"><i class="icon-rss2"></i></span><a href="/rss.xml" class="aside__menuList-item"><span lang="zh" class="font__ui">订阅 / RSS</span></a></div></aside><aside class="aside__right"></aside></section><div id="footer" style="text-align:center" lang="zh" class="font__ui">© <a href="/" rel="nofollow">hsfzxjy 的博客.</a>&nbsp;Powered by&nbsp;<a rel="nofollow" target="_blank" href="https://hexo.io">Hexo.</a>&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy/byak-hexo">Theme</a>&nbsp;by&nbsp;<a rel="nofollow" target="_blank" href="https://github.com/hsfzxjy">hsfzxjy</a>.<div style="margin-top:10px"><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011202001249" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/img/beian.png" style="float:left" loading="lazy"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤公网安备44011202001249号</span></a><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px" href="http://beian.miit.gov.cn/"><span style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备2020075702号-1</span></a></div></div></body><script src="/dist/js/main.js" async defer></script></html>